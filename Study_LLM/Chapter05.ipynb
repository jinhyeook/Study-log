{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53b08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"open_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c9ca3",
   "metadata": {},
   "source": [
    "## 그래프 DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc864d52",
   "metadata": {},
   "source": [
    "### 환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237660f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "working_dir = Path('working_directory')\n",
    "working_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd71d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 03:26:20.0022 - INFO - graphrag.cli.initialize - Initializing project at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\n"
     ]
    }
   ],
   "source": [
    "!graphrag init --root ./working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86069864",
   "metadata": {},
   "source": [
    "### 그래프 DB 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f98240",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = working_dir / 'input'\n",
    "input_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "source_path = r\"data\\How_to_invest_money.txt\"\n",
    "destination_path = r\"working_directory\\input\\How_to_invest_money.txt\"\n",
    "\n",
    "shutil.copy(source_path, destination_path)\n",
    "\n",
    "if os.path.exists(destination_path):\n",
    "    print(f\"파일이 {destination_path}에 성공적으로 복사됨\")\n",
    "else:\n",
    "    print(\"복사 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158293de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 03:28:05.0365 - INFO - graphrag.cli.index - Logging enabled at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\logs\\logs.txt\n",
      "2025-07-29 03:28:06.0302 - INFO - graphrag.index.validate_config - LLM Config Params Validated\n",
      "2025-07-29 03:28:06.0761 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated\n",
      "2025-07-29 03:28:06.0761 - INFO - graphrag.cli.index - Starting pipeline run. False\n",
      "2025-07-29 03:28:06.0762 - INFO - graphrag.cli.index - Using default configuration: {\n",
      "    \"root_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\",\n",
      "    \"models\": {\n",
      "        \"default_chat_model\": {\n",
      "            \"api_key\": \"==== REDACTED ====\",\n",
      "            \"auth_type\": \"api_key\",\n",
      "            \"type\": \"openai_chat\",\n",
      "            \"model\": \"gpt-4-turbo-preview\",\n",
      "            \"encoding_model\": \"cl100k_base\",\n",
      "            \"api_base\": null,\n",
      "            \"api_version\": null,\n",
      "            \"deployment_name\": null,\n",
      "            \"proxy\": null,\n",
      "            \"audience\": null,\n",
      "            \"model_supports_json\": true,\n",
      "            \"request_timeout\": 180.0,\n",
      "            \"tokens_per_minute\": \"auto\",\n",
      "            \"requests_per_minute\": \"auto\",\n",
      "            \"retry_strategy\": \"native\",\n",
      "            \"max_retries\": 10,\n",
      "            \"max_retry_wait\": 10.0,\n",
      "            \"concurrent_requests\": 25,\n",
      "            \"async_mode\": \"threaded\",\n",
      "            \"responses\": null,\n",
      "            \"max_tokens\": null,\n",
      "            \"temperature\": 0,\n",
      "            \"max_completion_tokens\": null,\n",
      "            \"reasoning_effort\": null,\n",
      "            \"top_p\": 1,\n",
      "            \"n\": 1,\n",
      "            \"frequency_penalty\": 0.0,\n",
      "            \"presence_penalty\": 0.0\n",
      "        },\n",
      "        \"default_embedding_model\": {\n",
      "            \"api_key\": \"==== REDACTED ====\",\n",
      "            \"auth_type\": \"api_key\",\n",
      "            \"type\": \"openai_embedding\",\n",
      "            \"model\": \"text-embedding-3-small\",\n",
      "            \"encoding_model\": \"cl100k_base\",\n",
      "            \"api_base\": null,\n",
      "            \"api_version\": null,\n",
      "            \"deployment_name\": null,\n",
      "            \"proxy\": null,\n",
      "            \"audience\": null,\n",
      "            \"model_supports_json\": true,\n",
      "            \"request_timeout\": 180.0,\n",
      "            \"tokens_per_minute\": null,\n",
      "            \"requests_per_minute\": null,\n",
      "            \"retry_strategy\": \"native\",\n",
      "            \"max_retries\": 10,\n",
      "            \"max_retry_wait\": 10.0,\n",
      "            \"concurrent_requests\": 25,\n",
      "            \"async_mode\": \"threaded\",\n",
      "            \"responses\": null,\n",
      "            \"max_tokens\": null,\n",
      "            \"temperature\": 0,\n",
      "            \"max_completion_tokens\": null,\n",
      "            \"reasoning_effort\": null,\n",
      "            \"top_p\": 1,\n",
      "            \"n\": 1,\n",
      "            \"frequency_penalty\": 0.0,\n",
      "            \"presence_penalty\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"input\": {\n",
      "        \"storage\": {\n",
      "            \"type\": \"file\",\n",
      "            \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\input\",\n",
      "            \"storage_account_blob_url\": null,\n",
      "            \"cosmosdb_account_url\": null\n",
      "        },\n",
      "        \"file_type\": \"text\",\n",
      "        \"encoding\": \"utf-8\",\n",
      "        \"file_pattern\": \".*\\\\.txt$\",\n",
      "        \"file_filter\": null,\n",
      "        \"text_column\": \"text\",\n",
      "        \"title_column\": null,\n",
      "        \"metadata\": null\n",
      "    },\n",
      "    \"chunks\": {\n",
      "        \"size\": 1200,\n",
      "        \"overlap\": 100,\n",
      "        \"group_by_columns\": [\n",
      "            \"id\"\n",
      "        ],\n",
      "        \"strategy\": \"tokens\",\n",
      "        \"encoding_model\": \"cl100k_base\",\n",
      "        \"prepend_metadata\": false,\n",
      "        \"chunk_size_includes_metadata\": false\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\output\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"outputs\": null,\n",
      "    \"update_index_output\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\update_output\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"cache\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"cache\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"reporting\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\logs\",\n",
      "        \"storage_account_blob_url\": null\n",
      "    },\n",
      "    \"vector_store\": {\n",
      "        \"default_vector_store\": {\n",
      "            \"type\": \"lancedb\",\n",
      "            \"db_uri\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\output\\\\lancedb\",\n",
      "            \"url\": null,\n",
      "            \"audience\": null,\n",
      "            \"container_name\": \"==== REDACTED ====\",\n",
      "            \"database_name\": null,\n",
      "            \"overwrite\": true\n",
      "        }\n",
      "    },\n",
      "    \"workflows\": null,\n",
      "    \"embed_text\": {\n",
      "        \"model_id\": \"default_embedding_model\",\n",
      "        \"vector_store_id\": \"default_vector_store\",\n",
      "        \"batch_size\": 16,\n",
      "        \"batch_max_tokens\": 8191,\n",
      "        \"names\": [\n",
      "            \"entity.description\",\n",
      "            \"community.full_content\",\n",
      "            \"text_unit.text\"\n",
      "        ],\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"extract_graph\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/extract_graph.txt\",\n",
      "        \"entity_types\": [\n",
      "            \"organization\",\n",
      "            \"person\",\n",
      "            \"geo\",\n",
      "            \"event\"\n",
      "        ],\n",
      "        \"max_gleanings\": 1,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"summarize_descriptions\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/summarize_descriptions.txt\",\n",
      "        \"max_length\": 500,\n",
      "        \"max_input_tokens\": 4000,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"extract_graph_nlp\": {\n",
      "        \"normalize_edge_weights\": true,\n",
      "        \"text_analyzer\": {\n",
      "            \"extractor_type\": \"regex_english\",\n",
      "            \"model_name\": \"en_core_web_md\",\n",
      "            \"max_word_length\": 15,\n",
      "            \"word_delimiter\": \" \",\n",
      "            \"include_named_entities\": true,\n",
      "            \"exclude_nouns\": [\n",
      "                \"stuff\",\n",
      "                \"thing\",\n",
      "                \"things\",\n",
      "                \"bunch\",\n",
      "                \"bit\",\n",
      "                \"bits\",\n",
      "                \"people\",\n",
      "                \"person\",\n",
      "                \"okay\",\n",
      "                \"hey\",\n",
      "                \"hi\",\n",
      "                \"hello\",\n",
      "                \"laughter\",\n",
      "                \"oh\"\n",
      "            ],\n",
      "            \"exclude_entity_tags\": [\n",
      "                \"DATE\"\n",
      "            ],\n",
      "            \"exclude_pos_tags\": [\n",
      "                \"DET\",\n",
      "                \"PRON\",\n",
      "                \"INTJ\",\n",
      "                \"X\"\n",
      "            ],\n",
      "            \"noun_phrase_tags\": [\n",
      "                \"PROPN\",\n",
      "                \"NOUNS\"\n",
      "            ],\n",
      "            \"noun_phrase_grammars\": {\n",
      "                \"PROPN,PROPN\": \"PROPN\",\n",
      "                \"NOUN,NOUN\": \"NOUNS\",\n",
      "                \"NOUNS,NOUN\": \"NOUNS\",\n",
      "                \"ADJ,ADJ\": \"ADJ\",\n",
      "                \"ADJ,NOUN\": \"NOUNS\"\n",
      "            }\n",
      "        },\n",
      "        \"concurrent_requests\": 25\n",
      "    },\n",
      "    \"prune_graph\": {\n",
      "        \"min_node_freq\": 2,\n",
      "        \"max_node_freq_std\": null,\n",
      "        \"min_node_degree\": 1,\n",
      "        \"max_node_degree_std\": null,\n",
      "        \"min_edge_weight_pct\": 40.0,\n",
      "        \"remove_ego_nodes\": true,\n",
      "        \"lcc_only\": false\n",
      "    },\n",
      "    \"cluster_graph\": {\n",
      "        \"max_cluster_size\": 10,\n",
      "        \"use_lcc\": true,\n",
      "        \"seed\": 3735928559\n",
      "    },\n",
      "    \"extract_claims\": {\n",
      "        \"enabled\": false,\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/extract_claims.txt\",\n",
      "        \"description\": \"Any claims or facts that could be relevant to information discovery.\",\n",
      "        \"max_gleanings\": 1,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"community_reports\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"graph_prompt\": \"prompts/community_report_graph.txt\",\n",
      "        \"text_prompt\": \"prompts/community_report_text.txt\",\n",
      "        \"max_length\": 2000,\n",
      "        \"max_input_length\": 8000,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"embed_graph\": {\n",
      "        \"enabled\": false,\n",
      "        \"dimensions\": 1536,\n",
      "        \"num_walks\": 10,\n",
      "        \"walk_length\": 40,\n",
      "        \"window_size\": 2,\n",
      "        \"iterations\": 3,\n",
      "        \"random_seed\": 597832,\n",
      "        \"use_lcc\": true\n",
      "    },\n",
      "    \"umap\": {\n",
      "        \"enabled\": false\n",
      "    },\n",
      "    \"snapshots\": {\n",
      "        \"embeddings\": false,\n",
      "        \"graphml\": false,\n",
      "        \"raw_graph\": false\n",
      "    },\n",
      "    \"local_search\": {\n",
      "        \"prompt\": \"prompts/local_search_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"text_unit_prop\": 0.5,\n",
      "        \"community_prop\": 0.15,\n",
      "        \"conversation_history_max_turns\": 5,\n",
      "        \"top_k_entities\": 10,\n",
      "        \"top_k_relationships\": 10,\n",
      "        \"max_context_tokens\": 12000\n",
      "    },\n",
      "    \"global_search\": {\n",
      "        \"map_prompt\": \"prompts/global_search_map_system_prompt.txt\",\n",
      "        \"reduce_prompt\": \"prompts/global_search_reduce_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"knowledge_prompt\": \"prompts/global_search_knowledge_system_prompt.txt\",\n",
      "        \"max_context_tokens\": 12000,\n",
      "        \"data_max_tokens\": 12000,\n",
      "        \"map_max_length\": 1000,\n",
      "        \"reduce_max_length\": 2000,\n",
      "        \"dynamic_search_threshold\": 1,\n",
      "        \"dynamic_search_keep_parent\": false,\n",
      "        \"dynamic_search_num_repeats\": 1,\n",
      "        \"dynamic_search_use_summary\": false,\n",
      "        \"dynamic_search_max_level\": 2\n",
      "    },\n",
      "    \"drift_search\": {\n",
      "        \"prompt\": \"prompts/drift_search_system_prompt.txt\",\n",
      "        \"reduce_prompt\": \"prompts/drift_search_reduce_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"data_max_tokens\": 12000,\n",
      "        \"reduce_max_tokens\": null,\n",
      "        \"reduce_temperature\": 0,\n",
      "        \"reduce_max_completion_tokens\": null,\n",
      "        \"concurrency\": 32,\n",
      "        \"drift_k_followups\": 20,\n",
      "        \"primer_folds\": 5,\n",
      "        \"primer_llm_max_tokens\": 12000,\n",
      "        \"n_depth\": 3,\n",
      "        \"local_search_text_unit_prop\": 0.9,\n",
      "        \"local_search_community_prop\": 0.1,\n",
      "        \"local_search_top_k_mapped_entities\": 10,\n",
      "        \"local_search_top_k_relationships\": 10,\n",
      "        \"local_search_max_data_tokens\": 12000,\n",
      "        \"local_search_temperature\": 0,\n",
      "        \"local_search_top_p\": 1,\n",
      "        \"local_search_n\": 1,\n",
      "        \"local_search_llm_max_gen_tokens\": null,\n",
      "        \"local_search_llm_max_gen_completion_tokens\": null\n",
      "    },\n",
      "    \"basic_search\": {\n",
      "        \"prompt\": \"prompts/basic_search_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"k\": 10,\n",
      "        \"max_context_tokens\": 12000\n",
      "    }\n",
      "}\n",
      "2025-07-29 03:28:06.0764 - INFO - graphrag.api.index - Initializing indexing pipeline...\n",
      "2025-07-29 03:28:06.0764 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']\n",
      "2025-07-29 03:28:06.0764 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input\n",
      "2025-07-29 03:28:06.0764 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\n",
      "2025-07-29 03:28:06.0766 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.\n",
      "2025-07-29 03:28:06.0768 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...\n",
      "2025-07-29 03:28:06.0769 - INFO - graphrag.index.input.factory - loading input from root_dir=C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input\n",
      "2025-07-29 03:28:06.0769 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text\n",
      "2025-07-29 03:28:06.0769 - INFO - graphrag.storage.file_pipeline_storage - search C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input for files matching .*\\.txt$\n",
      "2025-07-29 03:28:06.0772 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1\n",
      "2025-07-29 03:28:06.0773 - INFO - graphrag.index.input.util - Total number of unfiltered InputFileType.text rows: 1\n",
      "2025-07-29 03:28:06.0773 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1\n",
      "2025-07-29 03:28:06.0780 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully\n",
      "2025-07-29 03:28:06.0785 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units\n",
      "2025-07-29 03:28:06.0785 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 03:28:06.0801 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents\n",
      "2025-07-29 03:28:06.0820 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1\n",
      "2025-07-29 03:28:06.0827 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units\n",
      "2025-07-29 03:28:06.0828 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully\n",
      "2025-07-29 03:28:06.0833 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents\n",
      "2025-07-29 03:28:06.0833 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 03:28:06.0837 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 03:28:06.0858 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents\n",
      "2025-07-29 03:28:06.0858 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully\n",
      "2025-07-29 03:28:06.0862 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph\n",
      "2025-07-29 03:28:06.0863 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 03:28:22.0918 - INFO - graphrag.logger.progress - extract graph progress: 1/29\n",
      "2025-07-29 03:29:26.0750 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 146, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n",
      "    response = await self.model(prompt, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3048. Please try again in 6.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:26.0756 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 146, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n",
      "    response = await self.model(prompt, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3048. Please try again in 6.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:26.0758 - INFO - graphrag.logger.progress - extract graph progress: 2/29\n",
      "2025-07-29 03:29:30.0114 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3166. Please try again in 6.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:30.0120 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3166. Please try again in 6.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:30.0122 - INFO - graphrag.logger.progress - extract graph progress: 3/29\n",
      "2025-07-29 03:29:32.0409 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3280. Please try again in 6.56s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:32.0414 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3280. Please try again in 6.56s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:32.0416 - INFO - graphrag.logger.progress - extract graph progress: 4/29\n",
      "2025-07-29 03:29:33.0350 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3272. Please try again in 6.544s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:33.0354 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3272. Please try again in 6.544s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:33.0356 - INFO - graphrag.logger.progress - extract graph progress: 5/29\n",
      "2025-07-29 03:29:34.0258 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3247. Please try again in 6.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:34.0260 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3247. Please try again in 6.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:34.0262 - INFO - graphrag.logger.progress - extract graph progress: 6/29\n",
      "2025-07-29 03:29:37.0145 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3310. Please try again in 6.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:37.0150 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3310. Please try again in 6.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:37.0152 - INFO - graphrag.logger.progress - extract graph progress: 7/29\n",
      "2025-07-29 03:29:37.0252 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3363. Please try again in 6.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:37.0257 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3363. Please try again in 6.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:37.0261 - INFO - graphrag.logger.progress - extract graph progress: 8/29\n",
      "2025-07-29 03:29:38.0812 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3371. Please try again in 6.742s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:38.0818 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3371. Please try again in 6.742s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:38.0822 - INFO - graphrag.logger.progress - extract graph progress: 9/29\n",
      "2025-07-29 03:29:39.0202 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3414. Please try again in 6.828s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:39.0205 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3414. Please try again in 6.828s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:39.0206 - INFO - graphrag.logger.progress - extract graph progress: 10/29\n",
      "2025-07-29 03:29:39.0824 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3396. Please try again in 6.792s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:39.0827 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3396. Please try again in 6.792s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:39.0829 - INFO - graphrag.logger.progress - extract graph progress: 11/29\n",
      "2025-07-29 03:29:42.0349 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3412. Please try again in 6.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0351 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3412. Please try again in 6.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0353 - INFO - graphrag.logger.progress - extract graph progress: 12/29\n",
      "2025-07-29 03:29:42.0690 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3434. Please try again in 6.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0692 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3434. Please try again in 6.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0694 - INFO - graphrag.logger.progress - extract graph progress: 13/29\n",
      "2025-07-29 03:29:42.0927 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3475. Please try again in 6.95s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0931 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3475. Please try again in 6.95s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:42.0932 - INFO - graphrag.logger.progress - extract graph progress: 14/29\n",
      "2025-07-29 03:29:43.0940 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3497. Please try again in 6.994s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:43.0943 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3497. Please try again in 6.994s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:43.0944 - INFO - graphrag.logger.progress - extract graph progress: 15/29\n",
      "2025-07-29 03:29:43.0997 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3486. Please try again in 6.972s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0000 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3486. Please try again in 6.972s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0001 - INFO - graphrag.logger.progress - extract graph progress: 16/29\n",
      "2025-07-29 03:29:44.0321 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3596. Please try again in 7.192s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0324 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3596. Please try again in 7.192s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0326 - INFO - graphrag.logger.progress - extract graph progress: 17/29\n",
      "2025-07-29 03:29:44.0603 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3594. Please try again in 7.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0607 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3594. Please try again in 7.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0608 - INFO - graphrag.logger.progress - extract graph progress: 18/29\n",
      "2025-07-29 03:29:44.0850 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3603. Please try again in 7.206s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0853 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3603. Please try again in 7.206s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:44.0854 - INFO - graphrag.logger.progress - extract graph progress: 19/29\n",
      "2025-07-29 03:29:45.0569 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3520. Please try again in 7.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:45.0576 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3520. Please try again in 7.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:45.0580 - INFO - graphrag.logger.progress - extract graph progress: 20/29\n",
      "2025-07-29 03:29:47.0840 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3571. Please try again in 7.142s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:47.0847 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3571. Please try again in 7.142s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:47.0849 - INFO - graphrag.logger.progress - extract graph progress: 21/29\n",
      "2025-07-29 03:29:48.0188 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3548. Please try again in 7.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:48.0190 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3548. Please try again in 7.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:48.0191 - INFO - graphrag.logger.progress - extract graph progress: 22/29\n",
      "2025-07-29 03:29:52.0057 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3687. Please try again in 7.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:52.0061 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3687. Please try again in 7.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:29:52.0062 - INFO - graphrag.logger.progress - extract graph progress: 23/29\n",
      "2025-07-29 03:30:01.0260 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3899. Please try again in 7.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:01.0262 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3899. Please try again in 7.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:01.0263 - INFO - graphrag.logger.progress - extract graph progress: 24/29\n",
      "2025-07-29 03:30:01.0871 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3851. Please try again in 7.701s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:01.0876 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3851. Please try again in 7.701s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:01.0878 - INFO - graphrag.logger.progress - extract graph progress: 25/29\n",
      "2025-07-29 03:30:03.0992 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3924. Please try again in 7.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:03.0996 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3924. Please try again in 7.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 03:30:03.0997 - INFO - graphrag.logger.progress - extract graph progress: 26/29\n",
      "2025-07-29 03:30:33.0250 - INFO - graphrag.logger.progress - extract graph progress: 27/29\n",
      "2025-07-29 03:30:40.0777 - INFO - graphrag.logger.progress - extract graph progress: 28/29\n",
      "2025-07-29 03:30:47.0016 - INFO - graphrag.logger.progress - extract graph progress: 29/29\n",
      "2025-07-29 03:30:47.0043 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/33\n",
      "2025-07-29 03:30:47.0044 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/33\n",
      "2025-07-29 03:30:47.0044 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/33\n",
      "2025-07-29 03:30:47.0044 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/33\n",
      "2025-07-29 03:30:47.0044 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/33\n",
      "2025-07-29 03:30:47.0044 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 11/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 12/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 13/33\n",
      "2025-07-29 03:30:47.0045 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 14/33\n",
      "2025-07-29 03:30:56.0656 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 15/33\n",
      "2025-07-29 03:31:03.0023 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 16/33\n",
      "2025-07-29 03:31:05.0902 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 17/33\n",
      "2025-07-29 03:31:05.0903 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 18/33\n",
      "2025-07-29 03:31:05.0903 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 19/33\n",
      "2025-07-29 03:31:05.0903 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 20/33\n",
      "2025-07-29 03:31:05.0903 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 21/33\n",
      "2025-07-29 03:31:05.0903 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 22/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 23/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 24/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 25/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 26/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 27/33\n",
      "2025-07-29 03:31:05.0904 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 28/33\n",
      "2025-07-29 03:31:05.0905 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 29/33\n",
      "2025-07-29 03:31:05.0905 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 30/33\n",
      "2025-07-29 03:31:05.0905 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 31/33\n",
      "2025-07-29 03:31:05.0905 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 32/33\n",
      "2025-07-29 03:31:12.0072 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 33/33\n",
      "2025-07-29 03:31:12.0080 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph\n",
      "2025-07-29 03:31:12.0080 - INFO - graphrag.api.index - Workflow extract_graph completed successfully\n",
      "2025-07-29 03:31:12.0089 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph\n",
      "2025-07-29 03:31:12.0089 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 03:31:12.0100 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 03:31:12.0121 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph\n",
      "2025-07-29 03:31:12.0121 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully\n",
      "2025-07-29 03:31:12.0130 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates\n",
      "2025-07-29 03:31:12.0130 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates\n",
      "2025-07-29 03:31:12.0130 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully\n",
      "2025-07-29 03:31:12.0130 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities\n",
      "2025-07-29 03:31:12.0130 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 03:31:12.0140 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 03:31:12.0170 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities\n",
      "2025-07-29 03:31:12.0171 - INFO - graphrag.api.index - Workflow create_communities completed successfully\n",
      "2025-07-29 03:31:12.0176 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units\n",
      "2025-07-29 03:31:12.0177 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 03:31:12.0179 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 03:31:12.0182 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 03:31:12.0198 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units\n",
      "2025-07-29 03:31:12.0199 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully\n",
      "2025-07-29 03:31:12.0209 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports\n",
      "2025-07-29 03:31:12.0210 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 03:31:12.0213 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 03:31:12.0216 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet\n",
      "2025-07-29 03:31:12.0244 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 14\n",
      "2025-07-29 03:31:40.0309 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/3\n",
      "2025-07-29 03:32:02.0416 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 2/3\n",
      "2025-07-29 03:32:06.0707 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 3/3\n",
      "2025-07-29 03:32:06.0716 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports\n",
      "2025-07-29 03:32:06.0716 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully\n",
      "2025-07-29 03:32:06.0727 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings\n",
      "2025-07-29 03:32:06.0728 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 03:32:06.0746 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 03:32:06.0750 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 03:32:06.0762 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 03:32:06.0765 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet\n",
      "2025-07-29 03:32:06.0778 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings\n",
      "2025-07-29 03:32:06.0778 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding entity.description: default-entity-description\n",
      "2025-07-29 03:32:06.0785 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 03:32:06.0795 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 17 inputs via 17 snippets using 2 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 03:32:08.0026 - INFO - graphrag.logger.progress - generate embeddings progress: 1/2\n",
      "2025-07-29 03:32:08.0304 - INFO - graphrag.logger.progress - generate embeddings progress: 2/2\n",
      "2025-07-29 03:32:08.0531 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content\n",
      "2025-07-29 03:32:08.0532 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 03:32:08.0535 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 03:32:09.0297 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1\n",
      "2025-07-29 03:32:09.0318 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text\n",
      "2025-07-29 03:32:09.0320 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 03:32:09.0350 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 29 inputs via 29 snippets using 5 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 03:32:09.0970 - INFO - graphrag.logger.progress - generate embeddings progress: 1/5\n",
      "2025-07-29 03:32:10.0049 - INFO - graphrag.logger.progress - generate embeddings progress: 2/5\n",
      "2025-07-29 03:32:10.0225 - INFO - graphrag.logger.progress - generate embeddings progress: 3/5\n",
      "2025-07-29 03:32:10.0405 - INFO - graphrag.logger.progress - generate embeddings progress: 4/5\n",
      "2025-07-29 03:32:10.0555 - INFO - graphrag.logger.progress - generate embeddings progress: 5/5\n",
      "2025-07-29 03:32:10.0596 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow completed: generate_text_embeddings\n",
      "2025-07-29 03:32:10.0596 - INFO - graphrag.api.index - Workflow generate_text_embeddings completed successfully\n",
      "2025-07-29 03:32:10.0621 - INFO - graphrag.index.run.run_pipeline - Indexing pipeline complete.\n",
      "2025-07-29 03:32:10.0625 - INFO - graphrag.cli.index - All workflows completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28T18:32:08Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-entity-description.lance, it will be created\n",
      "[2025-07-28T18:32:09Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-community-full_content.lance, it will be created\n",
      "[2025-07-28T18:32:10Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-text_unit-text.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "!graphrag index --root ./working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b185c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query --query \"돈을 투자하는 방법은?\" --method local --root ./working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d630afb",
   "metadata": {},
   "source": [
    "## Graph RAG 질의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df48e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory')\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5debf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"output/entities.parquet\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439be0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"output/communities.parquet\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170982cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method global \\\n",
    "--query \"부동산 담보 대출(real-estate mortgages)의 위험 요소는 무엇인가?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method local \\\n",
    "--query \"부동산 담보 대출(real-estate mortgages)의 위험 요소는 무엇인가?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method global \\\n",
    "--query \"산업채권 평가  시 순유동자산(net quick assets)을 어떻게 분석해야 하나요\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f34e1",
   "metadata": {},
   "source": [
    "## GraphRAG(Neo4j + 랭체인) 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942690b2",
   "metadata": {},
   "source": [
    "### 지식그래프 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbb994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory')\n",
    "GRAPHRAG_FOLDER = r\"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\"\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e788f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'human_readable_id', 'community', 'level', 'parent', 'children', 'title', 'summary', 'full_content', 'rank', 'rating_explanation', 'findings', 'full_content_json', 'period', 'size']\n"
     ]
    }
   ],
   "source": [
    "df_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/community_reports.parquet\")\n",
    "print(df_check.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create constraint chunk_id if not  exists for (c: __Chunk__) require c.id is unique\n",
      "\n",
      "create constraint document_id if not exists for (d:__Document__) require d.id is unique\n",
      "\n",
      "create constraint community_id if not exists for (c:__Community__) require c.community is unique\n",
      "\n",
      "create constraint entity_title if not exists for (e:__Entity__) require e.name is unique\n",
      "\n",
      "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique\n",
      "\n",
      "create constraint covariate_title if not exists for (e:__Covariate__) require e.title is unique\n",
      "\n",
      "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\n",
      "{'_contains_updates': True, 'properties_set': 1}\n",
      "1 rows in 0.23665452003479004 s.\n",
      "{'_contains_updates': True, 'properties_set': 58}\n",
      "29 rows in 0.6263918876647949 s.\n",
      "{'_contains_updates': True, 'properties_set': 51}\n",
      "17 rows in 0.282318115234375 s.\n",
      "{'_contains_updates': True, 'properties_set': 90}\n",
      "18 rows in 0.2740023136138916 s.\n",
      "{'_contains_updates': True, 'properties_set': 10}\n",
      "5 rows in 0.3155221939086914 s.\n",
      "{'_contains_updates': True, 'properties_set': 65}\n",
      "5 rows in 0.27449607849121094 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 4. NODE\\nnode_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/node.parquet\", columns=[\\n    \"id\", \"human_readable_id\", \"title\", \"community\", \"level\", \"degree\", \"x\", \"y\"\\n])\\nnode_df[\"community\"] = \"Community\" + node_df[\"community\"].astype(str)\\nnode_df = node_df.dropna(subset=[\"title\"])  # replace 사용 방지\\n\\nnode_statement = \"\"\"\\n    WITH value, replace(value.title, \\'\"\\', \\'\\') AS clean_title\\n    MATCH (e : __Entity__) WHERE e.name = clean_title\\n    MERGE (c:__Community__ {community: value.community})\\n    MERGE (e)-[:IN_COMMUNITY]->(c)\\n\"\"\"\\nbatched_import(node_statement, node_df)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "NEO4J_URI= r\"neo4j+s://64ba1d93.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = r\"neo4j\"\n",
    "NEO4J_PASSWORD = r\"aii9OqmpymY37RWOR-sYTkK0ZxsFzN1Dkrsn19GA4YE\"\n",
    "NEO4J_DATABASE = r\"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def batched_import(statement, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Import a dataframe into Neo4j using a approach\n",
    "\n",
    "    Parameters : statement is the Cypher query to execute, df is the dataframe to\n",
    "    import, and batch_size is the number of rows to import in each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = df.iloc[start : min(start+batch_size, total)]\n",
    "        result = driver.execute_query(\n",
    "            \"UNWIND $rows AS value\" + statement,\n",
    "            rows = batch.to_dict(\"records\"),\n",
    "            database_ = NEO4J_DATABASE,\n",
    "        )\n",
    "        print(result.summary.counters)\n",
    "    print(f\"{total} rows in {time.time() - start_s} s.\")\n",
    "    return total\n",
    "\n",
    "\n",
    "statements = [\n",
    "    \"\\ncreate constraint chunk_id if not  exists for (c: __Chunk__) require c.id is unique\",\n",
    "    \"\\ncreate constraint document_id if not exists for (d:__Document__) require d.id is unique\",\n",
    "    \"\\ncreate constraint community_id if not exists for (c:__Community__) require c.community is unique\",\n",
    "    \"\\ncreate constraint entity_title if not exists for (e:__Entity__) require e.name is unique\",\n",
    "    \"\\ncreate constraint entity_id if not exists for (e:__Entity__) require e.id is unique\",\n",
    "    \"\\ncreate constraint covariate_title if not exists for (e:__Covariate__) require e.title is unique\",\n",
    "    \"\\ncreate constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\",\n",
    "    \"\\n\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    if len((statement or \"\").strip()) > 0:\n",
    "        print(statement)\n",
    "        driver.execute_query(statement)\n",
    "\n",
    "\n",
    "doc_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/documents.parquet\", columns=[\"id\",\"title\"])\n",
    "doc_statement = \"\"\"\n",
    "MERGE (d : __Document__ {id: value.id})\n",
    "    SET d += value {.title}\n",
    "\"\"\"\n",
    "batched_import(doc_statement, doc_df)\n",
    "\n",
    "text_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/text_units.parquet\", columns=[\"id\",\"text\",\"n_tokens\",\"document_ids\"])\n",
    "text_statement =\"\"\"\n",
    "    MERGE (c : __Chunk__ {id: value.id})\n",
    "    SET c += value {.text, .n_tokens}\n",
    "    WITH c, value\n",
    "    UNWIND value.document_ids AS document\n",
    "    MATCH (d:__Document__ {id: document})\n",
    "    MERGE (c)-[:PART_OF]->(d)\n",
    "\"\"\"\n",
    "batched_import(text_statement, text_df)\n",
    "\n",
    "entity_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/entities.parquet\", columns=[\"title\", \"type\",\"description\", \"human_readable_id\", \"id\", \"text_unit_ids\"])\n",
    "entity_statement =\"\"\"\n",
    "    MERGE (e : __Entity__ {id: value.id})\n",
    "    SET e.human_readable_id = value.human_readable_id, e.description = value.description, e.name = coalesce(replace(value.title, '\"', ''), 'Unknown')\n",
    "    WITH e, value\n",
    "    CALL apoc.create.addLabels(e, CASE WHEN coalesce(value.type, \"\") = \"\" THEN [] ELSE [apoc.text.upperCamelCase(replace(value.type, '\"', ''))] END) YIELD node\n",
    "    UNWIND value.text_unit_ids AS text_unit\n",
    "    MATCH (c : __Chunk__ {id: text_unit})\n",
    "    MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "batched_import(entity_statement, entity_df)\n",
    "\n",
    "\n",
    "# 1. RELATIONSHIP\n",
    "rel_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/relationships.parquet\", columns=[\n",
    "    \"id\", \"source\", \"target\", \"combined_degree\", \"weight\", \"human_readable_id\", \"description\", \"text_unit_ids\"\n",
    "])\n",
    "rel_df = rel_df.rename(columns={\"combined_degree\": \"rank\"})\n",
    "rel_df = rel_df.dropna(subset=[\"id\", \"source\", \"target\"])  # MERGE id null 방지\n",
    "\n",
    "rel_statement = \"\"\"\n",
    "    WITH replace(value.source, '\"', '') AS source_name,\n",
    "         replace(value.target, '\"', '') AS target_name,\n",
    "         value AS value\n",
    "    MATCH (source : __Entity__ {name: source_name})\n",
    "    MATCH (target : __Entity__ {name: target_name})\n",
    "    MERGE (source)-[rel:RELATED {id: value.id}]->(target)\n",
    "    SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}\n",
    "    RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "batched_import(rel_statement, rel_df)\n",
    "\n",
    "# 2. COMMUNITY\n",
    "community_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/communities.parquet\", columns=[\n",
    "    \"id\", \"level\", \"title\", \"text_unit_ids\", \"relationship_ids\"\n",
    "])\n",
    "community_df = community_df.dropna(subset=[\"title\"])  # MERGE 키 null 방지\n",
    "\n",
    "community_statement = \"\"\"\n",
    "    MERGE (c : __Community__ {community: value.title})\n",
    "    SET c.title = value.title, c.level = value.level\n",
    "    WITH c, value\n",
    "    UNWIND value.text_unit_ids as text_unit_id\n",
    "    MATCH (t : __Chunk__ {id: text_unit_id})\n",
    "    MERGE (c)-[:HAS_CHUNK]-> (t)\n",
    "    WITH distinct c, value\n",
    "    UNWIND value.relationship_ids as rel_id\n",
    "    MATCH (start : __Entity__)-[:RELATED {id: rel_id}]->(end: __Entity__)\n",
    "    MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "    MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "    RETURN count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "batched_import(community_statement, community_df)\n",
    "\n",
    "# 3. COMMUNITY REPORT\n",
    "community_report_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/community_reports.parquet\", columns=['id', 'community', 'level', 'title', 'summary', 'full_content', 'rank', 'rating_explanation', 'findings'])\n",
    "community_report_df[\"community\"] = \"Community\" + community_report_df[\"community\"].astype(str)\n",
    "\n",
    "community_report_statement = \"\"\"\n",
    "    MERGE (c:__Community__ {community: value.community})\n",
    "    SET c.level = value.level,\n",
    "        c.name = value.title,\n",
    "        c.rank = value.rank,\n",
    "        c.rank_explanation = value.rank_explanation,\n",
    "        c.full_content = value.full_content,\n",
    "        c.summary = value.summary\n",
    "    WITH c, value\n",
    "    UNWIND range(0, size(value.findings)-1) AS finding_idx\n",
    "    WITH c, value, finding_idx, value.findings[finding_idx] AS finding\n",
    "    MERGE (f:Finding {id: value.community + \"_\" + finding_idx})\n",
    "    MERGE (c)-[:HAS_FINDING]->(f)\n",
    "    SET f += finding\n",
    "\"\"\"\n",
    "batched_import(community_report_statement, community_report_df)\n",
    "\n",
    "'''\n",
    "# 4. NODE\n",
    "node_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/node.parquet\", columns=[\n",
    "    \"id\", \"human_readable_id\", \"title\", \"community\", \"level\", \"degree\", \"x\", \"y\"\n",
    "])\n",
    "node_df[\"community\"] = \"Community\" + node_df[\"community\"].astype(str)\n",
    "node_df = node_df.dropna(subset=[\"title\"])  # replace 사용 방지\n",
    "\n",
    "node_statement = \"\"\"\n",
    "    WITH value, replace(value.title, '\"', '') AS clean_title\n",
    "    MATCH (e : __Entity__) WHERE e.name = clean_title\n",
    "    MERGE (c:__Community__ {community: value.community})\n",
    "    MERGE (e)-[:IN_COMMUNITY]->(c)\n",
    "\"\"\"\n",
    "batched_import(node_statement, node_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bb096",
   "metadata": {},
   "source": [
    "### 질의 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0853190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimji\\AppData\\Local\\Temp\\ipykernel_39364\\4255506915.py:89: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response\n",
      "마일당 순수익(NET INCOME PER MILE)을 분석하기 위해서는, 단순히 수익을 마일로 나눈 값 그 자체를 평가하는 것뿐만 아니라, 이를 통해 다양한 경영 및 재무적인 시사점을 도출할 수 있어야 합니다. 다음과 같은 몇 가지 방법으로 분석을 진행할 수 있습니다:\n",
      "\n",
      "1. **비교 분석**: 동일 산업 내의 다른 회사들과 비교하여 경쟁력을 평가합니다. 마일당 순수익이 높다면, 이는 운영 효율성이 높거나 비용 절감에 성공했음을 나타낼 수 있습니다.\n",
      "\n",
      "2. **추세 분석**: 시간이 지남에 따라 마일당 순수익이 어떻게 변화하고 있는지를 살펴봅니다. 이는 사업의 개선 혹은 악화 추세를 파악할 수 있게 해주며, 장기적인 전략 수정의 필요성을 진단하는 데 유용합니다.\n",
      "\n",
      "3. **원가-수익 분석**: 마일당 발생하는 수익과 비용을 상세히 분석하여 어느 부분에서 비용 절감이 가능한지를 파악합니다. 이는 비용 구조를 개선하기 위한 실질적인 방안을 마련하는 데 기여할 수 있습니다.\n",
      "\n",
      "4. **운영 효율성 평가**: 마일당 순수익을 통해 전체적인 운영 효율성을 평가할 수 있습니다. 예를 들어, 수익성이 낮다면, 어떤 부분에서 비효율적인 운영이 발생하고 있는지를 검토할 필요가 있습니다.\n",
      "\n",
      "5. **재무적 전략 수립**: 마일당 순수익은 기업의 전반적인 재무 전략, 특히 투자 및 예산 편성에 중요한 역할을 할 수 있습니다. 예를 들어, 특정 노선의 수익성이 낮다면, 그 노선에 대한 투자를 줄이거나 운영 방식을 변경하는 등의 의사결정을 내릴 수 있습니다.\n",
      "\n",
      "마일당 순수익 분석은 기업의 재무 상태와 운영 전략을 개선하는 데 핵심적인 정보를 제공하며, 이를 통해 경쟁력을 유지하고 강화할 수 있는 방법을 모색할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 로컬 검색\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "\n",
    "NEO4J_URI= r\"neo4j+s://64ba1d93.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = r\"neo4j\"\n",
    "NEO4J_PASSWORD = r\"aii9OqmpymY37RWOR-sYTkK0ZxsFzN1Dkrsn19GA4YE\"\n",
    "NEO4J_DATABASE = r\"neo4j\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "graph = Neo4jVector.from_existing_graph(\n",
    "    embedding=embedding,\n",
    "    node_label=\"__Entity__\",\n",
    "    text_node_properties=[\"description\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    url=NEO4J_URI,                    # ✅ 수정\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_entity_context(entity_name):\n",
    "    context = {\"name\": entity_name}\n",
    "    try:\n",
    "        chunk_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})<-[:HAS_ENTITY]-(c:__Chunk__)\n",
    "            RETURN c.text AS text\n",
    "        \"\"\"\n",
    "        chunk_result = neo4j_graph.query(chunk_query, {\"entity_name\": entity_name})\n",
    "        context[\"text_chunks\"] = [r[\"text\"] for r in chunk_result] if chunk_result else [\"No text chunk available\"]\n",
    "\n",
    "        community_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})-[:IN_COMMUNITY]->(com:__Community__)\n",
    "            RETURN com.full_content AS report\n",
    "        \"\"\"\n",
    "        community_result = neo4j_graph.query(community_query, {\"entity_name\": entity_name})\n",
    "        context[\"community_reports\"] = [r[\"report\"] for r in community_result] if community_result else [\"No community report available\"]\n",
    "\n",
    "        related_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})-[:RELATED]->(related:__Entity__)\n",
    "            RETURN related.name AS name, related.description AS description\n",
    "        \"\"\"\n",
    "        related_result = neo4j_graph.query(related_query, {\"entity_name\": entity_name})\n",
    "        context[\"related_entities\"] = (\n",
    "            [{\"name\": r[\"name\"], \"decription\": r[\"description\"]} for r in related_result]\n",
    "            if related_result else []\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context[\"error\"] = f\"Error fetching context : {str(e)}\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def create_structured_context(all_contexts, query):\n",
    "    context_str = \"##질문과 관련된 엔티티 정보\\n\\n\"\n",
    "    context_str += \"아래는 질문에 답변하는 데 유용한 엔티티들의 구조화된 정보입니다.\\n\\n\"\n",
    "\n",
    "    for i, ctx in enumerate(all_contexts, 1):\n",
    "        context_str += f\"### 엔티티 {i}: {ctx['name']}\\n\"\n",
    "        context_str += f\"- **설명**: {ctx['description']}\\n\"\n",
    "        context_str += \"- **텍스트 청크**:\\n\"\n",
    "        for chunk in ctx['text_chunks']:\n",
    "            context_str += f\" - {chunk}\\n\"\n",
    "        context_str += \"- **커뮤니티 보고서**:\\n\"\n",
    "        for report in ctx['community_reports']:\n",
    "            context_str += f\" - {report}\\n\"\n",
    "        \n",
    "        if ctx['related_entities']:\n",
    "            context_str += \"- **관련 엔티티**:\\n\"\n",
    "            for rel in ctx['related_entities']:\n",
    "                context_str += f\" - {rel['name']} : {rel['decription']}\\n\"\n",
    "        else:\n",
    "            context_str += \"- **관련 엔티티** : 없음\\n\"\n",
    "        context_str += \"\\n\"\n",
    "    return context_str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)\n",
    "retriever = graph.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # ✅ 오타 수정\n",
    "\n",
    "query = \"마일당 순수익(NET INCOME PER MILE)을 어떻게 분석해야 하나요?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "all_contexts = []\n",
    "for result in results:\n",
    "    entity_name = result.metadata.get(\"name\", \"Unknown\")\n",
    "    description = result.page_content\n",
    "    context = fetch_entity_context(entity_name)\n",
    "    context[\"name\"] = entity_name\n",
    "    context[\"description\"] = description\n",
    "    all_contexts.append(context)\n",
    "\n",
    "context_str = create_structured_context(all_contexts, query)\n",
    "\n",
    "prompt = f\"아래 맥락에 기반해서, 주어진 질문에 한국어로 답하세요\\n\\n **질문** : {query}\\n\\n **맥락**:\\n{context_str}\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Final Response\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1fe9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing communities: 100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모르겠습니다. 제공된 데이터에서 책의 주제에 대한 정보는 포함되어 있지 않습니다. [Data: Reports (2,7,34,46,64,+more)]\n"
     ]
    }
   ],
   "source": [
    "# 글로벌 검색\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)\n",
    "\n",
    "MAP_SYSTEM_PROMPT = \"\"\"\n",
    "    --- 역할 ---\n",
    "    제공된 컨텍스트를 활용하여 사용자의 질문에 답하는 어시스턴트입니다.\n",
    "\n",
    "    --- 목표 ---\n",
    "    주정진 컨텍스트가 질문을 답하기에 적절하다면 질문에 대한 답을 한 뒤, \n",
    "    답변의 중요도 점수를 입력하여 JSON 형식으로 생성하세요\n",
    "    정보가 부족하다면 \"모르겠습니다\"라고 답하세요.\n",
    "\n",
    "    각 포인트는 다음을 포함해야 합니다.\n",
    "    - 답변 : 질문에 대한 답변\n",
    "    - 중요도 점수 : 0~100점 사이의 정수\n",
    "\n",
    "    데이터 참조 예:\n",
    "    \"예시 문장 [Data : Reports (2,7,64,46,34, +more)]\"\n",
    "    (한 참조에 5개 이상의 id는 \"+more\"를 사용)\n",
    "\n",
    "    출력 예:\n",
    "    {{\"Answer\" : \"답변 [Data : Reports (보고서들 id들)]\", \"score\":점수}}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", MAP_SYSTEM_PROMPT),\n",
    "        (\"human\", \"question : {question}\\n\\n context:{context}\"),\n",
    "    ]\n",
    ")\n",
    "map_chain = map_prompt|llm|StrOutputParser()\n",
    "\n",
    "\n",
    "REDUCE_SYSTEM_PROMPT = \"\"\"\n",
    "    --- 역할 ---\n",
    "    맵 단계에서 처리된 여러 결과를 종합하여 사용자의 지문에 답하는 어시스턴트입니다.\n",
    "\n",
    "    --- 목표 ---\n",
    "    제공된 맵 단계 결과를 바탕으로, 질문에 대한 종합적인 답변을 마크다운 형식으로 작성하세요\n",
    "    중요도 점수를 고려하여 핵심적인 결과 위주로 반영하며, 불필요한 세부 사항은 제외하세요.\n",
    "    핵심 포인트와 시사점을 포함하고, 정보가 부족한 경우 \"모르겠습니다.\"라고 답하세요\n",
    "\n",
    "    --- 맵 단계 결과 ---\n",
    "    {report_data}\n",
    "    데이터 참조 형식은 아래를 따르세요:\n",
    "    \" 예시 문장 [Data: Reports (2,7,34,46,64,+more)]\"\n",
    "    (참조 ID가 5개 이상일 경우 \"+more\" 사용)\n",
    "    대상 응답 길이 및 형식 : {response_type}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", REDUCE_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reduce_chain = reduce_prompt|llm|StrOutputParser()\n",
    "\n",
    "response_type: str = \"multiple paragraphs\"\n",
    "def global_retriever(query:str, level:int, response_type:str=response_type) -> str:\n",
    "    community_data = graph.query(\n",
    "        \"\"\"\n",
    "            MATCH (c:__Community__)\n",
    "            WHERE c.level = $level\n",
    "            RETURN c.full_context AS output\n",
    "        \"\"\",\n",
    "        params={\"level\":level},\n",
    "    )\n",
    "\n",
    "    intermediate_results = []\n",
    "    for community in tqdm(community_data, desc=\"Processing communities\"):\n",
    "        intermediate_response = map_chain.invoke({\"question\":query, \"context\":community[\"output\"]})\n",
    "        intermediate_results.append(intermediate_response)\n",
    "    final_response = reduce_chain.invoke(\n",
    "        {\n",
    "            \"report_data\" : intermediate_results,\n",
    "            \"question\" : query,\n",
    "            \"response_type\" : response_type,\n",
    "        }    \n",
    "    )\n",
    "    return final_response\n",
    "\n",
    "# print(global_retriever(\"마일당 순수익(NET INCOME PER MILE)을 어떻게 분석해야 하나요??\",1))\n",
    "print(global_retriever(\"이 책의 주제가 뭐야?\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ff8610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing communities: 100%|██████████| 10/10 [00:12<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모르겠습니다. 제공된 맵 단계 결과에서는 레포트의 핵심 주제를 파악할 수 있는 정보가 부족합니다. 각 결과에서 동일하게 \"모르겠습니다\"라는 응답이 반복되고 있으며, 중요한 정보나 힌트를 제공하지 않고 있습니다. 추가적인 맥락이나 데이터가 필요하다면, 다른 질문을 시도하거나 추가 정보를 제공해 주실 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(global_retriever(\"레포트에서 다루는 핵심 주제는 무엇인가요?\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5052e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "GRAPHRAG_FOLDER = r\"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\"\n",
    "\n",
    "community_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/communities.parquet\")\n",
    "entities_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/entities.parquet\")\n",
    "text_units_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/text_units.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3c49fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>parent</th>\n",
       "      <th>children</th>\n",
       "      <th>title</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>period</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7c9800ce-59c4-4a80-b18f-0a5de5bfec39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 0</td>\n",
       "      <td>[3c1625ef-ff3b-4823-b813-1eeea516444c, a9e4156...</td>\n",
       "      <td>[04da4aa4-3a2f-4ab2-be1e-5c624c92c973, 0d3e1f8...</td>\n",
       "      <td>[3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...</td>\n",
       "      <td>2025-07-27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ee1c6ca-c160-4231-b8be-0a000831e5f3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 1</td>\n",
       "      <td>[3053af77-d624-4184-b5c4-2df39616d8d2, f029798...</td>\n",
       "      <td>[168203ac-b7bb-4e5c-bfbe-6f33bf1421f4, 2eb5762...</td>\n",
       "      <td>[3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...</td>\n",
       "      <td>2025-07-27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a0b18b7-1cda-4d34-80d1-91454119742e</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 2</td>\n",
       "      <td>[39e7cf42-3ba6-42a4-9216-bf38d18ab448, d827bd7...</td>\n",
       "      <td>[63b2e9bb-4bb4-48b3-9933-0115d07dd1a7, 9ee7c94...</td>\n",
       "      <td>[7545e407be21a6c05c05035ab3d90113699580ec998fb...</td>\n",
       "      <td>2025-07-27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  ...      period  size\n",
       "0  7c9800ce-59c4-4a80-b18f-0a5de5bfec39                  0  ...  2025-07-27     4\n",
       "1  5ee1c6ca-c160-4231-b8be-0a000831e5f3                  1  ...  2025-07-27     3\n",
       "2  3a0b18b7-1cda-4d34-80d1-91454119742e                  2  ...  2025-07-27     4\n",
       "\n",
       "[3 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a1a1708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>frequency</th>\n",
       "      <th>degree</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a9e41568-4407-4f5e-ab61-1023a25f1b97</td>\n",
       "      <td>0</td>\n",
       "      <td>PROJECT GUTENBERG</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Project Gutenberg is a pioneering digital libr...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f229545c-7b63-4238-8e34-0999dd285ed0</td>\n",
       "      <td>1</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>GEO</td>\n",
       "      <td>The United States is a country recognized for ...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afb5b14c-9ad6-404d-a88e-ac8ac72635dc</td>\n",
       "      <td>2</td>\n",
       "      <td>GEORGE GARR HENRY</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Author of \"How to Invest Money\", Vice-Presiden...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ddf73ab-939e-4de9-bd0c-e10faceffd2b</td>\n",
       "      <td>3</td>\n",
       "      <td>JULIA NEUFELD</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Contributor to the production of the eBook \"Ho...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2a45468c-8b01-4f93-8d90-88d9d8606556</td>\n",
       "      <td>4</td>\n",
       "      <td>ONLINE DISTRIBUTED PROOFREADING TEAM</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Group involved in the production of the eBook ...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0aee33e0-7f89-49cb-818c-980079aaf8b9</td>\n",
       "      <td>5</td>\n",
       "      <td>INTERNET ARCHIVE/AMERICAN LIBRARIES</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Source of images for the eBook \"How to Invest ...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cd6b4392-861e-4d34-96a1-cb2d598b4540</td>\n",
       "      <td>6</td>\n",
       "      <td>FUNK &amp; WAGNALLS COMPANY</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Publisher of \"How to Invest Money\", located in...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a50e280e-a68a-4356-940a-0aacf399dfef</td>\n",
       "      <td>7</td>\n",
       "      <td>GUARANTY TRUST COMPANY OF NEW YORK</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Organization where George Garr Henry served as...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d9a3c4c6-6668-4c2d-bb85-a1c5d2991383</td>\n",
       "      <td>8</td>\n",
       "      <td>SYSTEM MAGAZINE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d827bd75-813d-4a1f-8b5d-7ebecca70fff</td>\n",
       "      <td>9</td>\n",
       "      <td>PROJECT GUTENBERG LITERARY ARCHIVE FOUNDATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>The Project Gutenberg Literary Archive Foundat...</td>\n",
       "      <td>[ad31c63c1ca6a1f679fbbfcad23e44035953fcb759fc4...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  ...  x  y\n",
       "0  a9e41568-4407-4f5e-ab61-1023a25f1b97                  0  ...  0  0\n",
       "1  f229545c-7b63-4238-8e34-0999dd285ed0                  1  ...  0  0\n",
       "2  afb5b14c-9ad6-404d-a88e-ac8ac72635dc                  2  ...  0  0\n",
       "3  0ddf73ab-939e-4de9-bd0c-e10faceffd2b                  3  ...  0  0\n",
       "4  2a45468c-8b01-4f93-8d90-88d9d8606556                  4  ...  0  0\n",
       "5  0aee33e0-7f89-49cb-818c-980079aaf8b9                  5  ...  0  0\n",
       "6  cd6b4392-861e-4d34-96a1-cb2d598b4540                  6  ...  0  0\n",
       "7  a50e280e-a68a-4356-940a-0aacf399dfef                  7  ...  0  0\n",
       "8  d9a3c4c6-6668-4c2d-bb85-a1c5d2991383                  8  ...  0  0\n",
       "9  d827bd75-813d-4a1f-8b5d-7ebecca70fff                  9  ...  0  0\n",
       "\n",
       "[10 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_check.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cef32a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>document_ids</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>covariate_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549303b4d8de9adc2722b88096c96047d333a0ae0b5369...</td>\n",
       "      <td>1</td>\n",
       "      <td>﻿The Project Gutenberg eBook of How to Invest ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>[a9e41568-4407-4f5e-ab61-1023a25f1b97, f229545...</td>\n",
       "      <td>[75f1b558-a2c5-45c7-b164-90d431c8bb23, 22f448a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>552d40b335e06d33643e78da48f7499a526f8fa1f27b57...</td>\n",
       "      <td>2</td>\n",
       "      <td>way in which to dispose of it. It is obviousl...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c024dc1d1b06f88ddd921300222ed0a569307e0b55a448...</td>\n",
       "      <td>3</td>\n",
       "      <td>\\nmore than obedience to the old rule which fo...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c071fb8607b6007d80519d742ec2a81ff896ef3e5e95f3...</td>\n",
       "      <td>4</td>\n",
       "      <td>terms of a lease, by the railroads\\nwhich use...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4bd9417eeedfa8493f4699924f3ee9a4324d5cfebb3c60...</td>\n",
       "      <td>5</td>\n",
       "      <td>ust.\\n\\n\\n\\n\\nII\\n\\nRAILROAD MORTGAGE BONDS\\n\\...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81b68038f6f230e5da687de75906f0f89021a61caa5c55...</td>\n",
       "      <td>6</td>\n",
       "      <td>, to mortgage bonds upon the\\ngeneral mileage ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47580f85d0dd81692165c417576f6c4cf7b015ec21838c...</td>\n",
       "      <td>7</td>\n",
       "      <td>, it does not always follow that its operating...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30ba7af0e052a9f08830b2853c6777672a88fcb6ae3ec7...</td>\n",
       "      <td>8</td>\n",
       "      <td>outstanding April 1st, 1908, at the market pr...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a2a423b48b790e6d9511ccd340618421bf574e7e866d55...</td>\n",
       "      <td>9</td>\n",
       "      <td>to one of two standard forms: (1) The conditi...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e7adb4654819efd85be45646189342bc909210a39f4c95...</td>\n",
       "      <td>10</td>\n",
       "      <td>both. Two of these railroads offered to the ho...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  ...  covariate_ids\n",
       "0  549303b4d8de9adc2722b88096c96047d333a0ae0b5369...  ...             []\n",
       "1  552d40b335e06d33643e78da48f7499a526f8fa1f27b57...  ...             []\n",
       "2  c024dc1d1b06f88ddd921300222ed0a569307e0b55a448...  ...             []\n",
       "3  c071fb8607b6007d80519d742ec2a81ff896ef3e5e95f3...  ...             []\n",
       "4  4bd9417eeedfa8493f4699924f3ee9a4324d5cfebb3c60...  ...             []\n",
       "5  81b68038f6f230e5da687de75906f0f89021a61caa5c55...  ...             []\n",
       "6  47580f85d0dd81692165c417576f6c4cf7b015ec21838c...  ...             []\n",
       "7  30ba7af0e052a9f08830b2853c6777672a88fcb6ae3ec7...  ...             []\n",
       "8  a2a423b48b790e6d9511ccd340618421bf574e7e866d55...  ...             []\n",
       "9  e7adb4654819efd85be45646189342bc909210a39f4c95...  ...             []\n",
       "\n",
       "[10 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_units_check.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80675415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
