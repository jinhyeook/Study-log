{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53b08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"open_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c9ca3",
   "metadata": {},
   "source": [
    "## 그래프 DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc864d52",
   "metadata": {},
   "source": [
    "### 환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237660f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "working_dir = Path('working_directory')\n",
    "working_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd71d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 22:02:57.0769 - INFO - graphrag.cli.initialize - Initializing project at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\n"
     ]
    }
   ],
   "source": [
    "!graphrag init --root ./working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86069864",
   "metadata": {},
   "source": [
    "### 그래프 DB 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f98240",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = working_dir/'input'\n",
    "input_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "source_path = r\"data\\How_to_invest_money.txt\"\n",
    "destination_path = r\"working_directory\\input\\How_to_invest_money.txt\"\n",
    "\n",
    "shutil.copy(source_path, destination_path)\n",
    "\n",
    "if os.path.exists(destination_path):\n",
    "    print(f\"파일이 {destination_path}에 성공적으로 복사됨\")\n",
    "else:\n",
    "    print(\"복사 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158293de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 22:03:54.0925 - INFO - graphrag.cli.index - Logging enabled at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\logs\\logs.txt\n",
      "2025-07-29 22:03:55.0988 - INFO - graphrag.index.validate_config - LLM Config Params Validated\n",
      "2025-07-29 22:03:57.0124 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated\n",
      "2025-07-29 22:03:57.0125 - INFO - graphrag.cli.index - Starting pipeline run. False\n",
      "2025-07-29 22:03:57.0126 - INFO - graphrag.cli.index - Using default configuration: {\n",
      "    \"root_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\",\n",
      "    \"models\": {\n",
      "        \"default_chat_model\": {\n",
      "            \"api_key\": \"==== REDACTED ====\",\n",
      "            \"auth_type\": \"api_key\",\n",
      "            \"type\": \"openai_chat\",\n",
      "            \"model\": \"gpt-4-turbo-preview\",\n",
      "            \"encoding_model\": \"cl100k_base\",\n",
      "            \"api_base\": null,\n",
      "            \"api_version\": null,\n",
      "            \"deployment_name\": null,\n",
      "            \"proxy\": null,\n",
      "            \"audience\": null,\n",
      "            \"model_supports_json\": true,\n",
      "            \"request_timeout\": 180.0,\n",
      "            \"tokens_per_minute\": \"auto\",\n",
      "            \"requests_per_minute\": \"auto\",\n",
      "            \"retry_strategy\": \"native\",\n",
      "            \"max_retries\": 10,\n",
      "            \"max_retry_wait\": 10.0,\n",
      "            \"concurrent_requests\": 25,\n",
      "            \"async_mode\": \"threaded\",\n",
      "            \"responses\": null,\n",
      "            \"max_tokens\": null,\n",
      "            \"temperature\": 0,\n",
      "            \"max_completion_tokens\": null,\n",
      "            \"reasoning_effort\": null,\n",
      "            \"top_p\": 1,\n",
      "            \"n\": 1,\n",
      "            \"frequency_penalty\": 0.0,\n",
      "            \"presence_penalty\": 0.0\n",
      "        },\n",
      "        \"default_embedding_model\": {\n",
      "            \"api_key\": \"==== REDACTED ====\",\n",
      "            \"auth_type\": \"api_key\",\n",
      "            \"type\": \"openai_embedding\",\n",
      "            \"model\": \"text-embedding-3-small\",\n",
      "            \"encoding_model\": \"cl100k_base\",\n",
      "            \"api_base\": null,\n",
      "            \"api_version\": null,\n",
      "            \"deployment_name\": null,\n",
      "            \"proxy\": null,\n",
      "            \"audience\": null,\n",
      "            \"model_supports_json\": true,\n",
      "            \"request_timeout\": 180.0,\n",
      "            \"tokens_per_minute\": null,\n",
      "            \"requests_per_minute\": null,\n",
      "            \"retry_strategy\": \"native\",\n",
      "            \"max_retries\": 10,\n",
      "            \"max_retry_wait\": 10.0,\n",
      "            \"concurrent_requests\": 25,\n",
      "            \"async_mode\": \"threaded\",\n",
      "            \"responses\": null,\n",
      "            \"max_tokens\": null,\n",
      "            \"temperature\": 0,\n",
      "            \"max_completion_tokens\": null,\n",
      "            \"reasoning_effort\": null,\n",
      "            \"top_p\": 1,\n",
      "            \"n\": 1,\n",
      "            \"frequency_penalty\": 0.0,\n",
      "            \"presence_penalty\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"input\": {\n",
      "        \"storage\": {\n",
      "            \"type\": \"file\",\n",
      "            \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\input\",\n",
      "            \"storage_account_blob_url\": null,\n",
      "            \"cosmosdb_account_url\": null\n",
      "        },\n",
      "        \"file_type\": \"text\",\n",
      "        \"encoding\": \"utf-8\",\n",
      "        \"file_pattern\": \".*\\\\.txt$\",\n",
      "        \"file_filter\": null,\n",
      "        \"text_column\": \"text\",\n",
      "        \"title_column\": null,\n",
      "        \"metadata\": null\n",
      "    },\n",
      "    \"chunks\": {\n",
      "        \"size\": 1200,\n",
      "        \"overlap\": 100,\n",
      "        \"group_by_columns\": [\n",
      "            \"id\"\n",
      "        ],\n",
      "        \"strategy\": \"tokens\",\n",
      "        \"encoding_model\": \"cl100k_base\",\n",
      "        \"prepend_metadata\": false,\n",
      "        \"chunk_size_includes_metadata\": false\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\output\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"outputs\": null,\n",
      "    \"update_index_output\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\update_output\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"cache\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"cache\",\n",
      "        \"storage_account_blob_url\": null,\n",
      "        \"cosmosdb_account_url\": null\n",
      "    },\n",
      "    \"reporting\": {\n",
      "        \"type\": \"file\",\n",
      "        \"base_dir\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\logs\",\n",
      "        \"storage_account_blob_url\": null\n",
      "    },\n",
      "    \"vector_store\": {\n",
      "        \"default_vector_store\": {\n",
      "            \"type\": \"lancedb\",\n",
      "            \"db_uri\": \"C:\\\\Users\\\\kimji\\\\Desktop\\\\ProgramFile\\\\Study\\\\Study_LLM\\\\working_directory\\\\output\\\\lancedb\",\n",
      "            \"url\": null,\n",
      "            \"audience\": null,\n",
      "            \"container_name\": \"==== REDACTED ====\",\n",
      "            \"database_name\": null,\n",
      "            \"overwrite\": true\n",
      "        }\n",
      "    },\n",
      "    \"workflows\": null,\n",
      "    \"embed_text\": {\n",
      "        \"model_id\": \"default_embedding_model\",\n",
      "        \"vector_store_id\": \"default_vector_store\",\n",
      "        \"batch_size\": 16,\n",
      "        \"batch_max_tokens\": 8191,\n",
      "        \"names\": [\n",
      "            \"entity.description\",\n",
      "            \"community.full_content\",\n",
      "            \"text_unit.text\"\n",
      "        ],\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"extract_graph\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/extract_graph.txt\",\n",
      "        \"entity_types\": [\n",
      "            \"organization\",\n",
      "            \"person\",\n",
      "            \"geo\",\n",
      "            \"event\"\n",
      "        ],\n",
      "        \"max_gleanings\": 1,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"summarize_descriptions\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/summarize_descriptions.txt\",\n",
      "        \"max_length\": 500,\n",
      "        \"max_input_tokens\": 4000,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"extract_graph_nlp\": {\n",
      "        \"normalize_edge_weights\": true,\n",
      "        \"text_analyzer\": {\n",
      "            \"extractor_type\": \"regex_english\",\n",
      "            \"model_name\": \"en_core_web_md\",\n",
      "            \"max_word_length\": 15,\n",
      "            \"word_delimiter\": \" \",\n",
      "            \"include_named_entities\": true,\n",
      "            \"exclude_nouns\": [\n",
      "                \"stuff\",\n",
      "                \"thing\",\n",
      "                \"things\",\n",
      "                \"bunch\",\n",
      "                \"bit\",\n",
      "                \"bits\",\n",
      "                \"people\",\n",
      "                \"person\",\n",
      "                \"okay\",\n",
      "                \"hey\",\n",
      "                \"hi\",\n",
      "                \"hello\",\n",
      "                \"laughter\",\n",
      "                \"oh\"\n",
      "            ],\n",
      "            \"exclude_entity_tags\": [\n",
      "                \"DATE\"\n",
      "            ],\n",
      "            \"exclude_pos_tags\": [\n",
      "                \"DET\",\n",
      "                \"PRON\",\n",
      "                \"INTJ\",\n",
      "                \"X\"\n",
      "            ],\n",
      "            \"noun_phrase_tags\": [\n",
      "                \"PROPN\",\n",
      "                \"NOUNS\"\n",
      "            ],\n",
      "            \"noun_phrase_grammars\": {\n",
      "                \"PROPN,PROPN\": \"PROPN\",\n",
      "                \"NOUN,NOUN\": \"NOUNS\",\n",
      "                \"NOUNS,NOUN\": \"NOUNS\",\n",
      "                \"ADJ,ADJ\": \"ADJ\",\n",
      "                \"ADJ,NOUN\": \"NOUNS\"\n",
      "            }\n",
      "        },\n",
      "        \"concurrent_requests\": 25\n",
      "    },\n",
      "    \"prune_graph\": {\n",
      "        \"min_node_freq\": 2,\n",
      "        \"max_node_freq_std\": null,\n",
      "        \"min_node_degree\": 1,\n",
      "        \"max_node_degree_std\": null,\n",
      "        \"min_edge_weight_pct\": 40.0,\n",
      "        \"remove_ego_nodes\": true,\n",
      "        \"lcc_only\": false\n",
      "    },\n",
      "    \"cluster_graph\": {\n",
      "        \"max_cluster_size\": 10,\n",
      "        \"use_lcc\": true,\n",
      "        \"seed\": 3735928559\n",
      "    },\n",
      "    \"extract_claims\": {\n",
      "        \"enabled\": false,\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"prompt\": \"prompts/extract_claims.txt\",\n",
      "        \"description\": \"Any claims or facts that could be relevant to information discovery.\",\n",
      "        \"max_gleanings\": 1,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"community_reports\": {\n",
      "        \"model_id\": \"default_chat_model\",\n",
      "        \"graph_prompt\": \"prompts/community_report_graph.txt\",\n",
      "        \"text_prompt\": \"prompts/community_report_text.txt\",\n",
      "        \"max_length\": 2000,\n",
      "        \"max_input_length\": 8000,\n",
      "        \"strategy\": null\n",
      "    },\n",
      "    \"embed_graph\": {\n",
      "        \"enabled\": true,\n",
      "        \"dimensions\": 1536,\n",
      "        \"num_walks\": 10,\n",
      "        \"walk_length\": 40,\n",
      "        \"window_size\": 2,\n",
      "        \"iterations\": 3,\n",
      "        \"random_seed\": 597832,\n",
      "        \"use_lcc\": true\n",
      "    },\n",
      "    \"umap\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"snapshots\": {\n",
      "        \"embeddings\": false,\n",
      "        \"graphml\": false,\n",
      "        \"raw_graph\": false\n",
      "    },\n",
      "    \"local_search\": {\n",
      "        \"prompt\": \"prompts/local_search_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"text_unit_prop\": 0.5,\n",
      "        \"community_prop\": 0.15,\n",
      "        \"conversation_history_max_turns\": 5,\n",
      "        \"top_k_entities\": 10,\n",
      "        \"top_k_relationships\": 10,\n",
      "        \"max_context_tokens\": 12000\n",
      "    },\n",
      "    \"global_search\": {\n",
      "        \"map_prompt\": \"prompts/global_search_map_system_prompt.txt\",\n",
      "        \"reduce_prompt\": \"prompts/global_search_reduce_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"knowledge_prompt\": \"prompts/global_search_knowledge_system_prompt.txt\",\n",
      "        \"max_context_tokens\": 12000,\n",
      "        \"data_max_tokens\": 12000,\n",
      "        \"map_max_length\": 1000,\n",
      "        \"reduce_max_length\": 2000,\n",
      "        \"dynamic_search_threshold\": 1,\n",
      "        \"dynamic_search_keep_parent\": false,\n",
      "        \"dynamic_search_num_repeats\": 1,\n",
      "        \"dynamic_search_use_summary\": false,\n",
      "        \"dynamic_search_max_level\": 2\n",
      "    },\n",
      "    \"drift_search\": {\n",
      "        \"prompt\": \"prompts/drift_search_system_prompt.txt\",\n",
      "        \"reduce_prompt\": \"prompts/drift_search_reduce_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"data_max_tokens\": 12000,\n",
      "        \"reduce_max_tokens\": null,\n",
      "        \"reduce_temperature\": 0,\n",
      "        \"reduce_max_completion_tokens\": null,\n",
      "        \"concurrency\": 32,\n",
      "        \"drift_k_followups\": 20,\n",
      "        \"primer_folds\": 5,\n",
      "        \"primer_llm_max_tokens\": 12000,\n",
      "        \"n_depth\": 3,\n",
      "        \"local_search_text_unit_prop\": 0.9,\n",
      "        \"local_search_community_prop\": 0.1,\n",
      "        \"local_search_top_k_mapped_entities\": 10,\n",
      "        \"local_search_top_k_relationships\": 10,\n",
      "        \"local_search_max_data_tokens\": 12000,\n",
      "        \"local_search_temperature\": 0,\n",
      "        \"local_search_top_p\": 1,\n",
      "        \"local_search_n\": 1,\n",
      "        \"local_search_llm_max_gen_tokens\": null,\n",
      "        \"local_search_llm_max_gen_completion_tokens\": null\n",
      "    },\n",
      "    \"basic_search\": {\n",
      "        \"prompt\": \"prompts/basic_search_system_prompt.txt\",\n",
      "        \"chat_model_id\": \"default_chat_model\",\n",
      "        \"embedding_model_id\": \"default_embedding_model\",\n",
      "        \"k\": 10,\n",
      "        \"max_context_tokens\": 12000\n",
      "    }\n",
      "}\n",
      "2025-07-29 22:03:57.0127 - INFO - graphrag.api.index - Initializing indexing pipeline...\n",
      "2025-07-29 22:03:57.0127 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']\n",
      "2025-07-29 22:03:57.0127 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input\n",
      "2025-07-29 22:03:57.0128 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\n",
      "2025-07-29 22:03:57.0129 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.\n",
      "2025-07-29 22:03:57.0132 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...\n",
      "2025-07-29 22:03:57.0132 - INFO - graphrag.index.input.factory - loading input from root_dir=C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input\n",
      "2025-07-29 22:03:57.0132 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text\n",
      "2025-07-29 22:03:57.0132 - INFO - graphrag.storage.file_pipeline_storage - search C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\input for files matching .*\\.txt$\n",
      "2025-07-29 22:03:57.0135 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1\n",
      "2025-07-29 22:03:57.0136 - INFO - graphrag.index.input.util - Total number of unfiltered InputFileType.text rows: 1\n",
      "2025-07-29 22:03:57.0136 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1\n",
      "2025-07-29 22:03:57.0158 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully\n",
      "2025-07-29 22:03:57.0168 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units\n",
      "2025-07-29 22:03:57.0168 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 22:03:57.0197 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents\n",
      "2025-07-29 22:03:57.0215 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1\n",
      "2025-07-29 22:03:57.0225 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units\n",
      "2025-07-29 22:03:57.0225 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully\n",
      "2025-07-29 22:03:57.0230 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents\n",
      "2025-07-29 22:03:57.0230 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 22:03:57.0233 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 22:03:57.0252 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents\n",
      "2025-07-29 22:03:57.0252 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully\n",
      "2025-07-29 22:03:57.0256 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph\n",
      "2025-07-29 22:03:57.0257 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 22:04:21.0168 - INFO - graphrag.logger.progress - extract graph progress: 1/29\n",
      "2025-07-29 22:05:25.0426 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 146, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n",
      "    response = await self.model(prompt, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3048. Please try again in 6.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:25.0467 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 146, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n",
      "    response = await self.model(prompt, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3048. Please try again in 6.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:25.0468 - INFO - graphrag.logger.progress - extract graph progress: 2/29\n",
      "2025-07-29 22:05:27.0440 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3184. Please try again in 6.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:27.0443 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3184. Please try again in 6.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:27.0444 - INFO - graphrag.logger.progress - extract graph progress: 3/29\n",
      "2025-07-29 22:05:27.0663 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3216. Please try again in 6.432s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:27.0664 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3216. Please try again in 6.432s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:27.0666 - INFO - graphrag.logger.progress - extract graph progress: 4/29\n",
      "2025-07-29 22:05:29.0050 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3229. Please try again in 6.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:29.0055 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3229. Please try again in 6.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:29.0058 - INFO - graphrag.logger.progress - extract graph progress: 5/29\n",
      "2025-07-29 22:05:31.0082 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3277. Please try again in 6.554s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:31.0087 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3277. Please try again in 6.554s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:31.0089 - INFO - graphrag.logger.progress - extract graph progress: 6/29\n",
      "2025-07-29 22:05:33.0982 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3301. Please try again in 6.602s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:33.0984 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3301. Please try again in 6.602s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:33.0985 - INFO - graphrag.logger.progress - extract graph progress: 7/29\n",
      "2025-07-29 22:05:35.0953 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3381. Please try again in 6.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:35.0954 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3381. Please try again in 6.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:35.0956 - INFO - graphrag.logger.progress - extract graph progress: 8/29\n",
      "2025-07-29 22:05:36.0325 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3358. Please try again in 6.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:36.0327 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3358. Please try again in 6.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:36.0328 - INFO - graphrag.logger.progress - extract graph progress: 9/29\n",
      "2025-07-29 22:05:37.0454 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3397. Please try again in 6.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:37.0456 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3397. Please try again in 6.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:37.0457 - INFO - graphrag.logger.progress - extract graph progress: 10/29\n",
      "2025-07-29 22:05:37.0687 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3359. Please try again in 6.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:37.0689 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3359. Please try again in 6.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:37.0690 - INFO - graphrag.logger.progress - extract graph progress: 11/29\n",
      "2025-07-29 22:05:39.0347 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3431. Please try again in 6.862s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:39.0350 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3431. Please try again in 6.862s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:39.0351 - INFO - graphrag.logger.progress - extract graph progress: 12/29\n",
      "2025-07-29 22:05:39.0789 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3394. Please try again in 6.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:39.0793 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3394. Please try again in 6.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:39.0795 - INFO - graphrag.logger.progress - extract graph progress: 13/29\n",
      "2025-07-29 22:05:41.0471 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3487. Please try again in 6.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:41.0474 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3487. Please try again in 6.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:41.0475 - INFO - graphrag.logger.progress - extract graph progress: 14/29\n",
      "2025-07-29 22:05:42.0575 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3492. Please try again in 6.984s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:42.0581 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3492. Please try again in 6.984s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:42.0584 - INFO - graphrag.logger.progress - extract graph progress: 15/29\n",
      "2025-07-29 22:05:45.0930 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3579. Please try again in 7.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:45.0932 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3579. Please try again in 7.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:45.0934 - INFO - graphrag.logger.progress - extract graph progress: 16/29\n",
      "2025-07-29 22:05:46.0447 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3507. Please try again in 7.014s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0450 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3507. Please try again in 7.014s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0453 - INFO - graphrag.logger.progress - extract graph progress: 17/29\n",
      "2025-07-29 22:05:46.0492 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3599. Please try again in 7.198s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0496 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3599. Please try again in 7.198s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0498 - INFO - graphrag.logger.progress - extract graph progress: 18/29\n",
      "2025-07-29 22:05:46.0816 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3642. Please try again in 7.284s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0820 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3642. Please try again in 7.284s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:46.0822 - INFO - graphrag.logger.progress - extract graph progress: 19/29\n",
      "2025-07-29 22:05:48.0342 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3609. Please try again in 7.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:48.0347 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3609. Please try again in 7.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:48.0349 - INFO - graphrag.logger.progress - extract graph progress: 20/29\n",
      "2025-07-29 22:05:48.0910 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3594. Please try again in 7.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:48.0913 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3594. Please try again in 7.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:48.0915 - INFO - graphrag.logger.progress - extract graph progress: 21/29\n",
      "2025-07-29 22:05:49.0117 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3675. Please try again in 7.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0121 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3675. Please try again in 7.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0123 - INFO - graphrag.logger.progress - extract graph progress: 22/29\n",
      "2025-07-29 22:05:49.0236 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3618. Please try again in 7.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0238 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3618. Please try again in 7.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0240 - INFO - graphrag.logger.progress - extract graph progress: 23/29\n",
      "2025-07-29 22:05:49.0954 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3797. Please try again in 7.594s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0957 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3797. Please try again in 7.594s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:49.0958 - INFO - graphrag.logger.progress - extract graph progress: 24/29\n",
      "2025-07-29 22:05:50.0367 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3672. Please try again in 7.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:50.0371 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3672. Please try again in 7.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:50.0372 - INFO - graphrag.logger.progress - extract graph progress: 25/29\n",
      "2025-07-29 22:05:51.0601 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3722. Please try again in 7.444s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:51.0607 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 118, in __call__\n",
      "    result = await self._process_document(text, prompt_variables)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\index\\operations\\extract_graph\\graph_extractor.py\", line 158, in _process_document\n",
      "    response = await self._model.achat(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 84, in achat\n",
      "    response = await self.model(prompt, history=history, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n",
      "    return await self._text_chat_llm(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n",
      "    return await self._delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n",
      "    return await self._decorated_target(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 78, in invoke\n",
      "    return await delegate(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n",
      "    result = await delegate(prompt, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n",
      "    result = await delegate(prompt, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n",
      "    output = await self._execute_llm(prompt, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 173, in _execute_llm\n",
      "    raw_response = await self._client.chat.completions.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\openai\\_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-UyShmZGnCG3UI8WWbfO7gG8B on tokens per min (TPM): Limit 30000, Used 30000, Requested 3722. Please try again in 7.444s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2025-07-29 22:05:51.0610 - INFO - graphrag.logger.progress - extract graph progress: 26/29\n",
      "2025-07-29 22:06:29.0723 - INFO - graphrag.logger.progress - extract graph progress: 27/29\n",
      "2025-07-29 22:06:36.0162 - INFO - graphrag.logger.progress - extract graph progress: 28/29\n",
      "2025-07-29 22:06:37.0650 - INFO - graphrag.logger.progress - extract graph progress: 29/29\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/30\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/30\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/30\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/30\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/30\n",
      "2025-07-29 22:06:37.0682 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 11/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 12/30\n",
      "2025-07-29 22:06:37.0683 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 13/30\n",
      "2025-07-29 22:06:48.0736 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 14/30\n",
      "2025-07-29 22:06:55.0110 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 15/30\n",
      "2025-07-29 22:06:58.0531 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 16/30\n",
      "2025-07-29 22:06:58.0532 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 17/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 18/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 19/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 20/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 21/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 22/30\n",
      "2025-07-29 22:06:58.0533 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 23/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 24/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 25/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 26/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 27/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 28/30\n",
      "2025-07-29 22:06:58.0534 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 29/30\n",
      "2025-07-29 22:07:07.0488 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 30/30\n",
      "2025-07-29 22:07:07.0496 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph\n",
      "2025-07-29 22:07:07.0496 - INFO - graphrag.api.index - Workflow extract_graph completed successfully\n",
      "2025-07-29 22:07:07.0507 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph\n",
      "2025-07-29 22:07:07.0507 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 22:07:07.0518 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 22:07:14.0514 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph\n",
      "2025-07-29 22:07:14.0514 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully\n",
      "2025-07-29 22:07:14.0524 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates\n",
      "2025-07-29 22:07:14.0524 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates\n",
      "2025-07-29 22:07:14.0524 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully\n",
      "2025-07-29 22:07:14.0524 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities\n",
      "2025-07-29 22:07:14.0525 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 22:07:14.0536 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 22:07:14.0569 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities\n",
      "2025-07-29 22:07:14.0569 - INFO - graphrag.api.index - Workflow create_communities completed successfully\n",
      "2025-07-29 22:07:14.0575 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units\n",
      "2025-07-29 22:07:14.0575 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 22:07:14.0578 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 22:07:14.0582 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 22:07:14.0596 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units\n",
      "2025-07-29 22:07:14.0597 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully\n",
      "2025-07-29 22:07:14.0607 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports\n",
      "2025-07-29 22:07:14.0607 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 22:07:14.0610 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 22:07:14.0612 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet\n",
      "2025-07-29 22:07:14.0644 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 10\n",
      "2025-07-29 22:07:36.0399 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/3\n",
      "2025-07-29 22:07:52.0873 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 2/3\n",
      "2025-07-29 22:08:03.0245 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 3/3\n",
      "2025-07-29 22:08:03.0257 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports\n",
      "2025-07-29 22:08:03.0257 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully\n",
      "2025-07-29 22:08:03.0265 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings\n",
      "2025-07-29 22:08:03.0266 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet\n",
      "2025-07-29 22:08:03.0278 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet\n",
      "2025-07-29 22:08:03.0281 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet\n",
      "2025-07-29 22:08:03.0290 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet\n",
      "2025-07-29 22:08:03.0292 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet\n",
      "2025-07-29 22:08:03.0304 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings\n",
      "2025-07-29 22:08:03.0304 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding entity.description: default-entity-description\n",
      "2025-07-29 22:08:03.0319 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 22:08:03.0328 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 16 inputs via 16 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 22:08:04.0030 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1\n",
      "2025-07-29 22:08:04.0311 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content\n",
      "2025-07-29 22:08:04.0313 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 22:08:04.0315 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 22:08:04.0791 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1\n",
      "2025-07-29 22:08:04.0812 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text\n",
      "2025-07-29 22:08:04.0814 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "2025-07-29 22:08:04.0840 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 29 inputs via 29 snippets using 5 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "2025-07-29 22:08:05.0349 - INFO - graphrag.logger.progress - generate embeddings progress: 1/5\n",
      "2025-07-29 22:08:05.0414 - INFO - graphrag.logger.progress - generate embeddings progress: 2/5\n",
      "2025-07-29 22:08:05.0443 - INFO - graphrag.logger.progress - generate embeddings progress: 3/5\n",
      "2025-07-29 22:08:05.0753 - INFO - graphrag.logger.progress - generate embeddings progress: 4/5\n",
      "2025-07-29 22:08:05.0815 - INFO - graphrag.logger.progress - generate embeddings progress: 5/5\n",
      "2025-07-29 22:08:05.0840 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow completed: generate_text_embeddings\n",
      "2025-07-29 22:08:05.0840 - INFO - graphrag.api.index - Workflow generate_text_embeddings completed successfully\n",
      "2025-07-29 22:08:05.0865 - INFO - graphrag.index.run.run_pipeline - Indexing pipeline complete.\n",
      "2025-07-29 22:08:05.0869 - INFO - graphrag.cli.index - All workflows completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\rag_env\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "INFO:graphrag.index.workflows.finalize_graph:Workflow completed: finalize_graph\n",
      "INFO:graphrag.api.index:Workflow finalize_graph completed successfully\n",
      "INFO:graphrag.index.workflows.extract_covariates:Workflow started: extract_covariates\n",
      "INFO:graphrag.index.workflows.extract_covariates:Workflow completed: extract_covariates\n",
      "INFO:graphrag.api.index:Workflow extract_covariates completed successfully\n",
      "INFO:graphrag.index.workflows.create_communities:Workflow started: create_communities\n",
      "INFO:graphrag.utils.storage:reading table from storage: entities.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: relationships.parquet\n",
      "INFO:graphrag.index.workflows.create_communities:Workflow completed: create_communities\n",
      "INFO:graphrag.api.index:Workflow create_communities completed successfully\n",
      "INFO:graphrag.index.workflows.create_final_text_units:Workflow started: create_final_text_units\n",
      "INFO:graphrag.utils.storage:reading table from storage: text_units.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: entities.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: relationships.parquet\n",
      "INFO:graphrag.index.workflows.create_final_text_units:Workflow completed: create_final_text_units\n",
      "INFO:graphrag.api.index:Workflow create_final_text_units completed successfully\n",
      "INFO:graphrag.index.workflows.create_community_reports:Workflow started: create_community_reports\n",
      "INFO:graphrag.utils.storage:reading table from storage: relationships.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: entities.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: communities.parquet\n",
      "INFO:graphrag.index.operations.summarize_communities.graph_context.context_builder:Number of nodes at level=0 => 10\n",
      "INFO:graphrag.logger.progress:level 0 summarize communities progress: 1/3\n",
      "INFO:graphrag.logger.progress:level 0 summarize communities progress: 2/3\n",
      "INFO:graphrag.logger.progress:level 0 summarize communities progress: 3/3\n",
      "INFO:graphrag.index.workflows.create_community_reports:Workflow completed: create_community_reports\n",
      "INFO:graphrag.api.index:Workflow create_community_reports completed successfully\n",
      "INFO:graphrag.index.workflows.generate_text_embeddings:Workflow started: generate_text_embeddings\n",
      "INFO:graphrag.utils.storage:reading table from storage: documents.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: relationships.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: text_units.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: entities.parquet\n",
      "INFO:graphrag.utils.storage:reading table from storage: community_reports.parquet\n",
      "INFO:graphrag.index.workflows.generate_text_embeddings:Creating embeddings\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:using vector store lancedb with container_name default for embedding entity.description: default-entity-description\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "INFO:graphrag.index.operations.embed_text.strategies.openai:embedding 16 inputs via 16 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 1/1\n",
      "[2025-07-29T13:08:04Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-entity-description.lance, it will be created\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "INFO:graphrag.index.operations.embed_text.strategies.openai:embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 1/1\n",
      "[2025-07-29T13:08:04Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-community-full_content.lance, it will be created\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text\n",
      "INFO:graphrag.index.operations.embed_text.embed_text:uploading text embeddings batch 1/1 of size 500 to vector store\n",
      "INFO:graphrag.index.operations.embed_text.strategies.openai:embedding 29 inputs via 29 snippets using 5 batches. max_batch_size=16, batch_max_tokens=8191\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 1/5\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 2/5\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 3/5\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 4/5\n",
      "INFO:graphrag.logger.progress:generate embeddings progress: 5/5\n",
      "[2025-07-29T13:08:05Z WARN  lance::dataset::write::insert] No existing dataset at C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\\lancedb\\default-text_unit-text.lance, it will be created\n",
      "INFO:graphrag.index.workflows.generate_text_embeddings:Workflow completed: generate_text_embeddings\n",
      "INFO:graphrag.api.index:Workflow generate_text_embeddings completed successfully\n",
      "INFO:graphrag.index.run.run_pipeline:Indexing pipeline complete.\n",
      "INFO:graphrag.cli.index:All workflows completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!graphrag index --root ./working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b185c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query --query \"돈을 투자하는 방법은?\" --method local --root ./working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d630afb",
   "metadata": {},
   "source": [
    "## Graph RAG 질의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df48e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory')\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5debf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"output/entities.parquet\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439be0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"output/communities.parquet\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170982cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method global \\\n",
    "--query \"부동산 담보 대출(real-estate mortgages)의 위험 요소는 무엇인가?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method local \\\n",
    "--query \"부동산 담보 대출(real-estate mortgages)의 위험 요소는 무엇인가?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!graphrag query\\\n",
    "--root ./ \\\n",
    "--method global \\\n",
    "--query \"산업채권 평가  시 순유동자산(net quick assets)을 어떻게 분석해야 하나요\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f34e1",
   "metadata": {},
   "source": [
    "## GraphRAG(Neo4j + 랭체인) 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942690b2",
   "metadata": {},
   "source": [
    "### 지식그래프 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbb994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory')\n",
    "GRAPHRAG_FOLDER = r\"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\"\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb529d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI= r\"neo4j+s://64ba1d93.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = r\"neo4j\"\n",
    "NEO4J_PASSWORD = r\"aii9OqmpymY37RWOR-sYTkK0ZxsFzN1Dkrsn19GA4YE\"\n",
    "NEO4J_DATABASE = r\"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "constraints = [\n",
    "    \"DROP CONSTRAINT chunk_id IF EXISTS\",\n",
    "    \"DROP CONSTRAINT document_id IF EXISTS\",\n",
    "    \"DROP CONSTRAINT community_id IF EXISTS\",\n",
    "    \"DROP CONSTRAINT entity_title IF EXISTS\",\n",
    "    \"DROP CONSTRAINT entity_id IF EXISTS\",\n",
    "    \"DROP CONSTRAINT covariate_title IF EXISTS\",\n",
    "    \"DROP CONSTRAINT related_id IF EXISTS\"\n",
    "]\n",
    "\n",
    "for stmt in constraints:\n",
    "    driver.execute_query(stmt, database_=NEO4J_DATABASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6b96ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x000002AA1AA2F690>, keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 노드 및 관계 삭제\n",
    "driver.execute_query(\"MATCH (n) DETACH DELETE n\", database_=NEO4J_DATABASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df87e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create constraint chunk_id if not  exists for (c: __Chunk__) require c.id is unique\n",
      "\n",
      "create constraint document_id if not exists for (d:__Document__) require d.id is unique\n",
      "\n",
      "create constraint community_id if not exists for (c:__Community__) require c.community is unique\n",
      "\n",
      "create constraint entity_title if not exists for (e:__Entity__) require e.name is unique\n",
      "\n",
      "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique\n",
      "\n",
      "create constraint covariate_title if not exists for (e:__Covariate__) require e.title is unique\n",
      "\n",
      "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\n",
      "{'_contains_updates': True, 'labels_added': 1, 'nodes_created': 1, 'properties_set': 2}\n",
      "1 rows in 0.2690107822418213 s.\n",
      "{'_contains_updates': True, 'labels_added': 29, 'relationships_created': 29, 'nodes_created': 29, 'properties_set': 87}\n",
      "29 rows in 0.6376347541809082 s.\n",
      "{'_contains_updates': True, 'labels_added': 16, 'relationships_created': 22, 'nodes_created': 16, 'properties_set': 64}\n",
      "16 rows in 0.3999905586242676 s.\n",
      "{'_contains_updates': True, 'relationships_created': 14, 'properties_set': 84}\n",
      "14 rows in 0.33658647537231445 s.\n",
      "{'_contains_updates': True, 'labels_added': 3, 'relationships_created': 13, 'nodes_created': 3, 'properties_set': 9}\n",
      "3 rows in 0.3959026336669922 s.\n",
      "{'_contains_updates': True, 'labels_added': 18, 'relationships_created': 15, 'nodes_created': 18, 'properties_set': 63}\n",
      "3 rows in 0.32175326347351074 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 4. NODE\\nnode_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/node.parquet\", columns=[\\n    \"id\", \"human_readable_id\", \"title\", \"community\", \"level\", \"degree\", \"x\", \"y\"\\n])\\nnode_df[\"community\"] = \"Community\" + node_df[\"community\"].astype(str)\\nnode_df = node_df.dropna(subset=[\"title\"])  # replace 사용 방지\\n\\nnode_statement = \"\"\"\\n    WITH value, replace(value.title, \\'\"\\', \\'\\') AS clean_title\\n    MATCH (e : __Entity__) WHERE e.name = clean_title\\n    MERGE (c:__Community__ {community: value.community})\\n    MERGE (e)-[:IN_COMMUNITY]->(c)\\n\"\"\"\\nbatched_import(node_statement, node_df)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "NEO4J_URI= r\"neo4j+s://64ba1d93.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = r\"neo4j\"\n",
    "NEO4J_PASSWORD = r\"aii9OqmpymY37RWOR-sYTkK0ZxsFzN1Dkrsn19GA4YE\"\n",
    "NEO4J_DATABASE = r\"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def batched_import(statement, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Import a dataframe into Neo4j using a approach\n",
    "\n",
    "    Parameters : statement is the Cypher query to execute, df is the dataframe to\n",
    "    import, and batch_size is the number of rows to import in each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = df.iloc[start : min(start+batch_size, total)]\n",
    "        result = driver.execute_query(\n",
    "            \"UNWIND $rows AS value\" + statement,\n",
    "            rows = batch.to_dict(\"records\"),\n",
    "            database_ = NEO4J_DATABASE,\n",
    "        )\n",
    "        print(result.summary.counters)\n",
    "    print(f\"{total} rows in {time.time() - start_s} s.\")\n",
    "    return total\n",
    "\n",
    "\n",
    "statements = [\n",
    "    \"\\ncreate constraint chunk_id if not  exists for (c: __Chunk__) require c.id is unique\",\n",
    "    \"\\ncreate constraint document_id if not exists for (d:__Document__) require d.id is unique\",\n",
    "    \"\\ncreate constraint community_id if not exists for (c:__Community__) require c.community is unique\",\n",
    "    \"\\ncreate constraint entity_title if not exists for (e:__Entity__) require e.name is unique\",\n",
    "    \"\\ncreate constraint entity_id if not exists for (e:__Entity__) require e.id is unique\",\n",
    "    \"\\ncreate constraint covariate_title if not exists for (e:__Covariate__) require e.title is unique\",\n",
    "    \"\\ncreate constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\",\n",
    "    \"\\n\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    if len((statement or \"\").strip()) > 0:\n",
    "        print(statement)\n",
    "        driver.execute_query(statement)\n",
    "\n",
    "\n",
    "doc_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/documents.parquet\", columns=[\"id\",\"title\"] )\n",
    "doc_statement = \"\"\"\n",
    "MERGE (d : __Document__ {id: value.id})\n",
    "    SET d += value {.title}\n",
    "\"\"\"\n",
    "batched_import(doc_statement, doc_df)\n",
    "\n",
    "text_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/text_units.parquet\", columns=[\"id\",\"text\",\"n_tokens\",\"document_ids\"])\n",
    "text_statement =\"\"\"\n",
    "    MERGE (c : __Chunk__ {id: value.id})\n",
    "    SET c += value {.text, .n_tokens}\n",
    "    WITH c, value\n",
    "    UNWIND value.document_ids AS document\n",
    "    MATCH (d:__Document__ {id: document})\n",
    "    MERGE (c)-[:PART_OF]->(d)\n",
    "\"\"\"\n",
    "batched_import(text_statement, text_df)\n",
    "\n",
    "entity_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/entities.parquet\", columns=[\"title\", \"type\",\"description\", \"human_readable_id\", \"id\", \"text_unit_ids\"])\n",
    "entity_statement =\"\"\"\n",
    "    MERGE (e : __Entity__ {id: value.id})\n",
    "    SET e.human_readable_id = value.human_readable_id, e.description = value.description, e.name = coalesce(replace(value.title, '\"', ''), 'Unknown')\n",
    "    WITH e, value\n",
    "    CALL apoc.create.addLabels(e, CASE WHEN coalesce(value.type, \"\") = \"\" THEN [] ELSE [apoc.text.upperCamelCase(replace(value.type, '\"', ''))] END) YIELD node\n",
    "    UNWIND value.text_unit_ids AS text_unit\n",
    "    MATCH (c : __Chunk__ {id: text_unit})\n",
    "    MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "batched_import(entity_statement, entity_df)\n",
    "\n",
    "\n",
    "# 1. RELATIONSHIP\n",
    "rel_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/relationships.parquet\", columns=[\n",
    "    \"id\", \"source\", \"target\", \"combined_degree\", \"weight\", \"human_readable_id\", \"description\", \"text_unit_ids\"\n",
    "])\n",
    "rel_df = rel_df.rename(columns={\"combined_degree\": \"rank\"})\n",
    "rel_df = rel_df.dropna(subset=[\"id\", \"source\", \"target\"])  # MERGE id null 방지\n",
    "\n",
    "rel_statement = \"\"\"\n",
    "    WITH replace(value.source, '\"', '') AS source_name,\n",
    "         replace(value.target, '\"', '') AS target_name,\n",
    "         value AS value\n",
    "    MATCH (source : __Entity__ {name: source_name})\n",
    "    MATCH (target : __Entity__ {name: target_name})\n",
    "    MERGE (source)-[rel:RELATED {id: value.id}]->(target)\n",
    "    SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}\n",
    "    RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "batched_import(rel_statement, rel_df)\n",
    "\n",
    "# 2. COMMUNITY\n",
    "community_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/communities.parquet\", columns=[\n",
    "    \"id\", \"level\", \"title\", \"text_unit_ids\", \"relationship_ids\"\n",
    "])\n",
    "community_df = community_df.dropna(subset=[\"title\"])  # MERGE 키 null 방지\n",
    "\n",
    "community_statement = \"\"\"\n",
    "    MERGE (c : __Community__ {community: value.title})\n",
    "    SET c.title = value.title, c.level = value.level\n",
    "    WITH c, value\n",
    "    UNWIND value.text_unit_ids as text_unit_id\n",
    "    MATCH (t : __Chunk__ {id: text_unit_id})\n",
    "    MERGE (c)-[:HAS_CHUNK]-> (t)\n",
    "    WITH distinct c, value\n",
    "    UNWIND value.relationship_ids as rel_id\n",
    "    MATCH (start : __Entity__)-[:RELATED {id: rel_id}]->(end: __Entity__)\n",
    "    MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "    MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "    RETURN count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "batched_import(community_statement, community_df)\n",
    "\n",
    "# 3. COMMUNITY REPORT\n",
    "community_report_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/community_reports.parquet\", columns=['id', 'community', 'level', 'title', 'summary', 'full_content', 'rank', 'rating_explanation', 'findings'])\n",
    "community_report_df[\"community\"] = \"Community\" + community_report_df[\"community\"].astype(str)\n",
    "\n",
    "community_report_statement = \"\"\"\n",
    "    MERGE (c:__Community__ {community: value.community})\n",
    "    SET c.level = value.level,\n",
    "        c.name = value.title,\n",
    "        c.rank = value.rank,\n",
    "        c.rank_explanation = value.rank_explanation,\n",
    "        c.full_content = value.full_content,\n",
    "        c.summary = value.summary\n",
    "    WITH c, value\n",
    "    UNWIND range(0, size(value.findings)-1) AS finding_idx\n",
    "    WITH c, value, finding_idx, value.findings[finding_idx] AS finding\n",
    "    MERGE (f:Finding {id: value.community + \"_\" + finding_idx})\n",
    "    MERGE (c)-[:HAS_FINDING]->(f)\n",
    "    SET f += finding\n",
    "\"\"\"\n",
    "batched_import(community_report_statement, community_report_df)\n",
    "\n",
    "'''\n",
    "# 4. NODE\n",
    "node_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/node.parquet\", columns=[\n",
    "    \"id\", \"human_readable_id\", \"title\", \"community\", \"level\", \"degree\", \"x\", \"y\"\n",
    "])\n",
    "node_df[\"community\"] = \"Community\" + node_df[\"community\"].astype(str)\n",
    "node_df = node_df.dropna(subset=[\"title\"])  # replace 사용 방지\n",
    "\n",
    "node_statement = \"\"\"\n",
    "    WITH value, replace(value.title, '\"', '') AS clean_title\n",
    "    MATCH (e : __Entity__) WHERE e.name = clean_title\n",
    "    MERGE (c:__Community__ {community: value.community})\n",
    "    MERGE (e)-[:IN_COMMUNITY]->(c)\n",
    "\"\"\"\n",
    "batched_import(node_statement, node_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bb096",
   "metadata": {},
   "source": [
    "### 질의 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0853190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimji\\AppData\\Local\\Temp\\ipykernel_17228\\3418269247.py:90: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response\n",
      "마일당 순수익을 분석하기 위해서는 다양한 요소를 고려해야 합니다. 먼저, 수익과 비용을 명확히 정의하고 계산해야 합니다. 수익은 주로 화물 운송이나 승객 운송에서 발생하는 수익을 포함하며, 비용은 연료비, 인건비, 유지보수비, 차량 감가상각 등을 포함합니다. \n",
      "\n",
      "이후 총 수익에서 총 비용을 차감하여 총 순수익을 계산한 다음, 이를 총 마일 수로 나누어 마일당 순수익을 산출합니다. \n",
      "\n",
      "또한, 이를 분석할 때는 시간에 따른 추세를 파악하기 위해 기간별로 데이터를 비교하거나, 경쟁사나 업계 평균과 비교하여 자사의 경쟁력을 평가하는 것도 중요합니다. 이러한 분석 과정에서 특정 마일에서 왜 수익이 증가하거나 감소했는지를 이해하고, 필요한 경우 비용 절감이나 수익 증대를 위한 전략을 수립하게 됩니다. \n",
      "\n",
      "마지막으로, 외부 경제 요인들, 예를 들어 연료 가격의 변동이나 정책 변화 등이 수익성에 어떤 영향을 미치는지 평가해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 로컬 검색\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "\n",
    "NEO4J_URI= r\"neo4j+s://64ba1d93.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = r\"neo4j\"\n",
    "NEO4J_PASSWORD = r\"aii9OqmpymY37RWOR-sYTkK0ZxsFzN1Dkrsn19GA4YE\"\n",
    "NEO4J_DATABASE = r\"neo4j\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "graph = Neo4jVector.from_existing_graph(\n",
    "    embedding=embedding,\n",
    "    node_label=\"__Entity__\",\n",
    "    text_node_properties=[\"description\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    url=NEO4J_URI,                  \n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_entity_context(entity_name):\n",
    "    context = {\"name\": entity_name}\n",
    "    try:\n",
    "        chunk_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})<-[:HAS_ENTITY]-(c:__Chunk__)\n",
    "            RETURN c.text AS text\n",
    "        \"\"\"\n",
    "        chunk_result = neo4j_graph.query(chunk_query, {\"entity_name\": entity_name})\n",
    "        context[\"text_chunks\"] = [r[\"text\"] for r in chunk_result] if chunk_result else [\"No text chunk available\"]\n",
    "\n",
    "        community_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})-[:IN_COMMUNITY]->(com:__Community__)\n",
    "            RETURN com.full_content AS report\n",
    "        \"\"\"\n",
    "        community_result = neo4j_graph.query(community_query, {\"entity_name\": entity_name})\n",
    "        context[\"community_reports\"] = [r[\"report\"] for r in community_result] if community_result else [\"No community report available\"]\n",
    "\n",
    "        related_query = \"\"\"\n",
    "            MATCH (e:__Entity__ {name: $entity_name})-[:RELATED]->(related:__Entity__)\n",
    "            RETURN related.name AS name, related.description AS description\n",
    "        \"\"\"\n",
    "        related_result = neo4j_graph.query(related_query, {\"entity_name\": entity_name})\n",
    "        context[\"related_entities\"] = (\n",
    "            [{\"name\": r[\"name\"], \"decription\": r[\"description\"]} for r in related_result]\n",
    "            if related_result else []\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context[\"error\"] = f\"Error fetching context : {str(e)}\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def create_structured_context(all_contexts, query):\n",
    "    context_str = \"##질문과 관련된 엔티티 정보\\n\\n\"\n",
    "    context_str += \"아래는 질문에 답변하는 데 유용한 엔티티들의 구조화된 정보입니다.\\n\\n\"\n",
    "\n",
    "    for i, ctx in enumerate(all_contexts, 1):\n",
    "        context_str += f\"### 엔티티 {i}: {ctx['name']}\\n\"\n",
    "        context_str += f\"- **설명**: {ctx['description']}\\n\"\n",
    "        context_str += \"- **텍스트 청크**:\\n\"\n",
    "        for chunk in ctx['text_chunks']:\n",
    "            context_str += f\" - {chunk}\\n\"\n",
    "        context_str += \"- **커뮤니티 보고서**:\\n\"\n",
    "        for report in ctx['community_reports']:\n",
    "            context_str += f\" - {report}\\n\"\n",
    "        \n",
    "        if ctx['related_entities']:\n",
    "            context_str += \"- **관련 엔티티**:\\n\"\n",
    "            for rel in ctx['related_entities']:\n",
    "                context_str += f\" - {rel['name']} : {rel['decription']}\\n\"\n",
    "        else:\n",
    "            context_str += \"- **관련 엔티티** : 없음\\n\"\n",
    "        context_str += \"\\n\"\n",
    "    return context_str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)\n",
    "retriever = graph.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # ✅ 오타 수정\n",
    "\n",
    "query = \"마일당 순수익(NET INCOME PER MILE)을 어떻게 분석해야 하나요?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "all_contexts = []\n",
    "for result in results:\n",
    "    entity_name = result.metadata.get(\"name\", \"Unknown\")\n",
    "    description = result.page_content\n",
    "    context = fetch_entity_context(entity_name)\n",
    "    context[\"name\"] = entity_name\n",
    "    context[\"description\"] = description\n",
    "    all_contexts.append(context)\n",
    "\n",
    "context_str = create_structured_context(all_contexts, query)\n",
    "\n",
    "prompt = f\"아래 맥락에 기반해서, 주어진 질문에 한국어로 답하세요\\n\\n **질문** : {query}\\n\\n **맥락**:\\n{context_str}\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Final Response\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1fe9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: full_context)} {position: line: 4, column: 22, offset: 93} for query: '\\n            MATCH (c:__Community__)\\n            WHERE c.level = $level\\n            RETURN c.full_context AS output\\n        '\n",
      "Processing communities: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모르겠습니다. 제공된 데이터를 바탕으로 이 책의 주제를 파악할 수 있는 정보가 없습니다. 추가적인 자료가 있으면 기꺼이 도와드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 글로벌 검색\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)\n",
    "\n",
    "MAP_SYSTEM_PROMPT = \"\"\"\n",
    "    --- 역할 ---\n",
    "    제공된 컨텍스트를 활용하여 사용자의 질문에 답하는 어시스턴트입니다.\n",
    "\n",
    "    --- 목표 ---\n",
    "    주정진 컨텍스트가 질문을 답하기에 적절하다면 질문에 대한 답을 한 뒤, \n",
    "    답변의 중요도 점수를 입력하여 JSON 형식으로 생성하세요\n",
    "    정보가 부족하다면 \"모르겠습니다\"라고 답하세요.\n",
    "\n",
    "    각 포인트는 다음을 포함해야 합니다.\n",
    "    - 답변 : 질문에 대한 답변\n",
    "    - 중요도 점수 : 0~100점 사이의 정수\n",
    "\n",
    "    데이터 참조 예:\n",
    "    \"예시 문장 [Data : Reports (2,7,64,46,34, +more)]\"\n",
    "    (한 참조에 5개 이상의 id는 \"+more\"를 사용)\n",
    "\n",
    "    출력 예:\n",
    "    {{\"Answer\" : \"답변 [Data : Reports (보고서들 id들)]\", \"score\":점수}}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", MAP_SYSTEM_PROMPT),\n",
    "        (\"human\", \"question : {question}\\n\\n context:{context}\"),\n",
    "    ]\n",
    ")\n",
    "map_chain = map_prompt|llm|StrOutputParser()\n",
    "\n",
    "\n",
    "REDUCE_SYSTEM_PROMPT = \"\"\"\n",
    "    --- 역할 ---\n",
    "    맵 단계에서 처리된 여러 결과를 종합하여 사용자의 지문에 답하는 어시스턴트입니다.\n",
    "\n",
    "    --- 목표 ---\n",
    "    제공된 맵 단계 결과를 바탕으로, 질문에 대한 종합적인 답변을 마크다운 형식으로 작성하세요\n",
    "    중요도 점수를 고려하여 핵심적인 결과 위주로 반영하며, 불필요한 세부 사항은 제외하세요.\n",
    "    핵심 포인트와 시사점을 포함하고, 정보가 부족한 경우 \"모르겠습니다.\"라고 답하세요\n",
    "\n",
    "    --- 맵 단계 결과 ---\n",
    "    {report_data}\n",
    "    데이터 참조 형식은 아래를 따르세요:\n",
    "    \" 예시 문장 [Data: Reports (2,7,34,46,64,+more)]\"\n",
    "    (참조 ID가 5개 이상일 경우 \"+more\" 사용)\n",
    "    대상 응답 길이 및 형식 : {response_type}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", REDUCE_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reduce_chain = reduce_prompt|llm|StrOutputParser()\n",
    "\n",
    "response_type: str = \"multiple paragraphs\"\n",
    "def global_retriever(query:str, level:int, response_type:str=response_type) -> str:\n",
    "    community_data = graph.query(\n",
    "        \"\"\"\n",
    "            MATCH (c:__Community__)\n",
    "            WHERE c.level = $level\n",
    "            RETURN c.full_context AS output\n",
    "        \"\"\",\n",
    "        params={\"level\":level},\n",
    "    )\n",
    "\n",
    "    intermediate_results = []\n",
    "    for community in tqdm(community_data, desc=\"Processing communities\"):\n",
    "        intermediate_response = map_chain.invoke({\"question\":query, \"context\":community[\"output\"]})\n",
    "        intermediate_results.append(intermediate_response)\n",
    "    final_response = reduce_chain.invoke(\n",
    "        {\n",
    "            \"report_data\" : intermediate_results,\n",
    "            \"question\" : query,\n",
    "            \"response_type\" : response_type,\n",
    "        }    \n",
    "    )\n",
    "    return final_response\n",
    "\n",
    "# print(global_retriever(\"마일당 순수익(NET INCOME PER MILE)을 어떻게 분석해야 하나요??\",1))\n",
    "print(global_retriever(\"이 책의 주제가 뭐야?\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff8610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: full_context)} {position: line: 4, column: 22, offset: 93} for query: '\\n            MATCH (c:__Community__)\\n            WHERE c.level = $level\\n            RETURN c.full_context AS output\\n        '\n",
      "Processing communities: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모르겠습니다. 제공된 정보를 바탕으로 레포트에서 다루는 핵심 주제를 구체적으로 파악할 수 없습니다. 추가적인 정보나 구체적인 데이터가 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(global_retriever(\"레포트에서 다루는 핵심 주제는 무엇인가요?\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5052e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "GRAPHRAG_FOLDER = r\"C:\\Users\\kimji\\Desktop\\ProgramFile\\Study\\Study_LLM\\working_directory\\output\"\n",
    "\n",
    "community_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/communities.parquet\")\n",
    "entities_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/entities.parquet\")\n",
    "text_units_check = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/text_units.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c49fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>parent</th>\n",
       "      <th>children</th>\n",
       "      <th>title</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>period</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7721efc7-864b-47e5-bfb7-6e242633cb90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 0</td>\n",
       "      <td>[4c1181d5-9fa6-4690-986d-4010e1ed661e, 1fa7b4f...</td>\n",
       "      <td>[30c09862-3701-41b3-9695-f146ffc61bda, e0ec4a7...</td>\n",
       "      <td>[3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...</td>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8577641d-1ff9-49b8-b343-16f2d9f26a27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 1</td>\n",
       "      <td>[896a6881-1d73-4cac-87e2-bc9db1e5de48, e0cd2bc...</td>\n",
       "      <td>[79b648fa-b1e6-4b31-863f-5ed6bedd26dd, cb7497f...</td>\n",
       "      <td>[7545e407be21a6c05c05035ab3d90113699580ec998fb...</td>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7dfe7b68-e087-445d-9f01-e536cbeb2b60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Community 2</td>\n",
       "      <td>[44664a45-6d60-49b0-aaed-54ee1ffa57d6, bf9509f...</td>\n",
       "      <td>[18f7f83e-d79c-4769-8dcf-7d9fd8c9d086, 72e5df5...</td>\n",
       "      <td>[3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...</td>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  community  level  \\\n",
       "0  7721efc7-864b-47e5-bfb7-6e242633cb90                  0          0      0   \n",
       "1  8577641d-1ff9-49b8-b343-16f2d9f26a27                  1          1      0   \n",
       "2  7dfe7b68-e087-445d-9f01-e536cbeb2b60                  2          2      0   \n",
       "\n",
       "   parent children        title  \\\n",
       "0      -1       []  Community 0   \n",
       "1      -1       []  Community 1   \n",
       "2      -1       []  Community 2   \n",
       "\n",
       "                                          entity_ids  \\\n",
       "0  [4c1181d5-9fa6-4690-986d-4010e1ed661e, 1fa7b4f...   \n",
       "1  [896a6881-1d73-4cac-87e2-bc9db1e5de48, e0cd2bc...   \n",
       "2  [44664a45-6d60-49b0-aaed-54ee1ffa57d6, bf9509f...   \n",
       "\n",
       "                                    relationship_ids  \\\n",
       "0  [30c09862-3701-41b3-9695-f146ffc61bda, e0ec4a7...   \n",
       "1  [79b648fa-b1e6-4b31-863f-5ed6bedd26dd, cb7497f...   \n",
       "2  [18f7f83e-d79c-4769-8dcf-7d9fd8c9d086, 72e5df5...   \n",
       "\n",
       "                                       text_unit_ids      period  size  \n",
       "0  [3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...  2025-07-29     3  \n",
       "1  [7545e407be21a6c05c05035ab3d90113699580ec998fb...  2025-07-29     4  \n",
       "2  [3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d...  2025-07-29     3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a1a1708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>frequency</th>\n",
       "      <th>degree</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf9509fe-2d8b-429c-9a43-79c0d497b785</td>\n",
       "      <td>0</td>\n",
       "      <td>PROJECT GUTENBERG</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Project Gutenberg is a pioneering digital libr...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17.980831</td>\n",
       "      <td>6.132166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abcd315d-a2da-45f6-ac01-baf8ac5dcdda</td>\n",
       "      <td>1</td>\n",
       "      <td>GEORGE GARR HENRY</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Author of \"How to Invest Money\", Vice-Presiden...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6d15953a-53ae-4c41-8fde-a085c6fd3f4d</td>\n",
       "      <td>2</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>GEO</td>\n",
       "      <td>The United States is a country where Project G...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17.496731</td>\n",
       "      <td>4.252052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>688c8c71-fbd9-491c-9111-2366f7192ede</td>\n",
       "      <td>3</td>\n",
       "      <td>JULIA NEUFELD</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Contributor to the production of the eBook \"Ho...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d9d38309-ac41-4972-b5d1-5a62b039c382</td>\n",
       "      <td>4</td>\n",
       "      <td>ONLINE DISTRIBUTED PROOFREADING TEAM</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Group involved in the production of \"How to In...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>e24dd398-2718-441a-b833-68486bb1f373</td>\n",
       "      <td>5</td>\n",
       "      <td>INTERNET ARCHIVE/AMERICAN LIBRARIES</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Source of images used for producing the eBook ...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6ca7f6fe-ad6b-45e5-b13e-1d96e86b45e1</td>\n",
       "      <td>6</td>\n",
       "      <td>FUNK &amp; WAGNALLS COMPANY</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Publisher of \"How to Invest Money\", located in...</td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aecea348-7f67-4907-89f6-bb554dd617c1</td>\n",
       "      <td>7</td>\n",
       "      <td>GUARANTY TRUST COMPANY OF NEW YORK</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[549303b4d8de9adc2722b88096c96047d333a0ae0b536...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e0cd2bc9-c5bf-4c3c-9ad7-15229ecce2c8</td>\n",
       "      <td>8</td>\n",
       "      <td>PROJECT GUTENBERG LITERARY ARCHIVE FOUNDATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>The Project Gutenberg Literary Archive Foundat...</td>\n",
       "      <td>[ad31c63c1ca6a1f679fbbfcad23e44035953fcb759fc4...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16.869572</td>\n",
       "      <td>6.582345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>896a6881-1d73-4cac-87e2-bc9db1e5de48</td>\n",
       "      <td>9</td>\n",
       "      <td>INTERNAL REVENUE SERVICE (IRS)</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>The U.S. federal agency responsible for tax co...</td>\n",
       "      <td>[7545e407be21a6c05c05035ab3d90113699580ec998fb...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.738295</td>\n",
       "      <td>5.064562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  bf9509fe-2d8b-429c-9a43-79c0d497b785                  0   \n",
       "1  abcd315d-a2da-45f6-ac01-baf8ac5dcdda                  1   \n",
       "2  6d15953a-53ae-4c41-8fde-a085c6fd3f4d                  2   \n",
       "3  688c8c71-fbd9-491c-9111-2366f7192ede                  3   \n",
       "4  d9d38309-ac41-4972-b5d1-5a62b039c382                  4   \n",
       "5  e24dd398-2718-441a-b833-68486bb1f373                  5   \n",
       "6  6ca7f6fe-ad6b-45e5-b13e-1d96e86b45e1                  6   \n",
       "7  aecea348-7f67-4907-89f6-bb554dd617c1                  7   \n",
       "8  e0cd2bc9-c5bf-4c3c-9ad7-15229ecce2c8                  8   \n",
       "9  896a6881-1d73-4cac-87e2-bc9db1e5de48                  9   \n",
       "\n",
       "                                           title          type  \\\n",
       "0                              PROJECT GUTENBERG  ORGANIZATION   \n",
       "1                              GEORGE GARR HENRY        PERSON   \n",
       "2                                  UNITED STATES           GEO   \n",
       "3                                  JULIA NEUFELD        PERSON   \n",
       "4           ONLINE DISTRIBUTED PROOFREADING TEAM  ORGANIZATION   \n",
       "5            INTERNET ARCHIVE/AMERICAN LIBRARIES  ORGANIZATION   \n",
       "6                        FUNK & WAGNALLS COMPANY  ORGANIZATION   \n",
       "7             GUARANTY TRUST COMPANY OF NEW YORK                 \n",
       "8  PROJECT GUTENBERG LITERARY ARCHIVE FOUNDATION  ORGANIZATION   \n",
       "9                 INTERNAL REVENUE SERVICE (IRS)  ORGANIZATION   \n",
       "\n",
       "                                         description  \\\n",
       "0  Project Gutenberg is a pioneering digital libr...   \n",
       "1  Author of \"How to Invest Money\", Vice-Presiden...   \n",
       "2  The United States is a country where Project G...   \n",
       "3  Contributor to the production of the eBook \"Ho...   \n",
       "4  Group involved in the production of \"How to In...   \n",
       "5  Source of images used for producing the eBook ...   \n",
       "6  Publisher of \"How to Invest Money\", located in...   \n",
       "7                                                      \n",
       "8  The Project Gutenberg Literary Archive Foundat...   \n",
       "9  The U.S. federal agency responsible for tax co...   \n",
       "\n",
       "                                       text_unit_ids  frequency  degree  \\\n",
       "0  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          4       4   \n",
       "1  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       2   \n",
       "2  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          2       2   \n",
       "3  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       1   \n",
       "4  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       2   \n",
       "5  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       1   \n",
       "6  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       1   \n",
       "7  [549303b4d8de9adc2722b88096c96047d333a0ae0b536...          1       1   \n",
       "8  [ad31c63c1ca6a1f679fbbfcad23e44035953fcb759fc4...          3       4   \n",
       "9  [7545e407be21a6c05c05035ab3d90113699580ec998fb...          1       1   \n",
       "\n",
       "           x         y  \n",
       "0  17.980831  6.132166  \n",
       "1        NaN       NaN  \n",
       "2  17.496731  4.252052  \n",
       "3        NaN       NaN  \n",
       "4        NaN       NaN  \n",
       "5        NaN       NaN  \n",
       "6        NaN       NaN  \n",
       "7        NaN       NaN  \n",
       "8  16.869572  6.582345  \n",
       "9  18.738295  5.064562  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_check.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef32a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>document_ids</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>covariate_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549303b4d8de9adc2722b88096c96047d333a0ae0b5369...</td>\n",
       "      <td>1</td>\n",
       "      <td>﻿The Project Gutenberg eBook of How to Invest ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>[bf9509fe-2d8b-429c-9a43-79c0d497b785, abcd315...</td>\n",
       "      <td>[4931c2ec-2b8c-4c38-97b2-d5ccd2647e6b, 33d0a3e...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>552d40b335e06d33643e78da48f7499a526f8fa1f27b57...</td>\n",
       "      <td>2</td>\n",
       "      <td>way in which to dispose of it. It is obviousl...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c024dc1d1b06f88ddd921300222ed0a569307e0b55a448...</td>\n",
       "      <td>3</td>\n",
       "      <td>\\nmore than obedience to the old rule which fo...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c071fb8607b6007d80519d742ec2a81ff896ef3e5e95f3...</td>\n",
       "      <td>4</td>\n",
       "      <td>terms of a lease, by the railroads\\nwhich use...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4bd9417eeedfa8493f4699924f3ee9a4324d5cfebb3c60...</td>\n",
       "      <td>5</td>\n",
       "      <td>ust.\\n\\n\\n\\n\\nII\\n\\nRAILROAD MORTGAGE BONDS\\n\\...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81b68038f6f230e5da687de75906f0f89021a61caa5c55...</td>\n",
       "      <td>6</td>\n",
       "      <td>, to mortgage bonds upon the\\ngeneral mileage ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47580f85d0dd81692165c417576f6c4cf7b015ec21838c...</td>\n",
       "      <td>7</td>\n",
       "      <td>, it does not always follow that its operating...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30ba7af0e052a9f08830b2853c6777672a88fcb6ae3ec7...</td>\n",
       "      <td>8</td>\n",
       "      <td>outstanding April 1st, 1908, at the market pr...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a2a423b48b790e6d9511ccd340618421bf574e7e866d55...</td>\n",
       "      <td>9</td>\n",
       "      <td>to one of two standard forms: (1) The conditi...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e7adb4654819efd85be45646189342bc909210a39f4c95...</td>\n",
       "      <td>10</td>\n",
       "      <td>both. Two of these railroads offered to the ho...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>78f47fc443a6a2604aae03e57a5aa1f49b76b2b51b7383...</td>\n",
       "      <td>11</td>\n",
       "      <td>-1/4-per-cent loss in principal. If it were ne...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>156d6e82c11ac3103488c6a3fc447ad0e7298b51658d9a...</td>\n",
       "      <td>12</td>\n",
       "      <td>such\\nincrease goes directly to the benefit o...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70b1bfed90e242fa52c3994e704045825393b7bd5c7efe...</td>\n",
       "      <td>13</td>\n",
       "      <td>in quoted values are\\nof no great importance ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>f17468e0cda42033623b8944e6f772ffcedf89d7043214...</td>\n",
       "      <td>14</td>\n",
       "      <td>, irrespective of the buildings and machinery ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20f6d70a1845feeac80bcc3ada4c2808cc5a17989bb255...</td>\n",
       "      <td>15</td>\n",
       "      <td>grows, to\\naccumulate the additional amount n...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3516d752d4e2e9650243cd7c60ae6c1fe0d7dff3b7341b...</td>\n",
       "      <td>16</td>\n",
       "      <td>vement in intrinsic value--are all inherent ch...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6b0e4f7bbc8158a123cabf889a5eaa21984ff984653071...</td>\n",
       "      <td>17</td>\n",
       "      <td>to make a careful valuation of the other phys...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>d4050c0d097eb36fe541c3bd14dc4b4025db8399231738...</td>\n",
       "      <td>18</td>\n",
       "      <td>be learned whether any real ground of content...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c6a8fc1e4491134e42c2c6ba630434d33b7d7414c5b9a3...</td>\n",
       "      <td>19</td>\n",
       "      <td>years of depression. At the same time,\\nit sh...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24c77ef5b5fdff898d1b09fb813b3ea3613a0b98752de3...</td>\n",
       "      <td>20</td>\n",
       "      <td>the tax lien is the foundation of the prime p...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1cff7a8de1d7c42c38392088bc42a5577ee64bc1bdf217...</td>\n",
       "      <td>21</td>\n",
       "      <td>to the requirements of the business surplus? ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>67833d1f76c62bca92b938fb7d680b0ec2303719cf76b3...</td>\n",
       "      <td>22</td>\n",
       "      <td>-managers attempted to give their stock an inv...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bd8f828f9196b7c7c8b5b1bbb130c31c419e9105ac99ba...</td>\n",
       "      <td>23</td>\n",
       "      <td>swings of prices. If an investor acquires suc...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9f3a4fddd9b13bb2af5edc2c844d6b38cbb038f9330661...</td>\n",
       "      <td>24</td>\n",
       "      <td>produce general swings in prices which may be ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7b58e4c9b1b81e0032bf09cf3305c73e39ba151af40602...</td>\n",
       "      <td>25</td>\n",
       "      <td>the excess finds its way into bank vaults\\nan...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>253082c58a8e7cf7330cf38296e620108f02d6a05f6eb3...</td>\n",
       "      <td>26</td>\n",
       "      <td>license, including paying royalties for use\\n...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ad31c63c1ca6a1f679fbbfcad23e44035953fcb759fc47...</td>\n",
       "      <td>27</td>\n",
       "      <td>charges. If you are\\nredistributing or provid...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>[bf9509fe-2d8b-429c-9a43-79c0d497b785, e0cd2bc...</td>\n",
       "      <td>[c936fe32-7873-40d7-b326-db8d8d66dc62]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7545e407be21a6c05c05035ab3d90113699580ec998fb1...</td>\n",
       "      <td>28</td>\n",
       "      <td>, DISCLAIMER OF DAMAGES - Except for the “Righ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>[bf9509fe-2d8b-429c-9a43-79c0d497b785, e0cd2bc...</td>\n",
       "      <td>[e0a4b47d-ab3c-421f-ad02-6e484c3c1b38, cb7497f...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d9...</td>\n",
       "      <td>29</td>\n",
       "      <td>machine-readable form accessible by the wides...</td>\n",
       "      <td>467</td>\n",
       "      <td>[76b874f8ddf312c5c5d34517a4587852e8ef79438600b...</td>\n",
       "      <td>[bf9509fe-2d8b-429c-9a43-79c0d497b785, 6d15953...</td>\n",
       "      <td>[c936fe32-7873-40d7-b326-db8d8d66dc62, 30c0986...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   id  human_readable_id  \\\n",
       "0   549303b4d8de9adc2722b88096c96047d333a0ae0b5369...                  1   \n",
       "1   552d40b335e06d33643e78da48f7499a526f8fa1f27b57...                  2   \n",
       "2   c024dc1d1b06f88ddd921300222ed0a569307e0b55a448...                  3   \n",
       "3   c071fb8607b6007d80519d742ec2a81ff896ef3e5e95f3...                  4   \n",
       "4   4bd9417eeedfa8493f4699924f3ee9a4324d5cfebb3c60...                  5   \n",
       "5   81b68038f6f230e5da687de75906f0f89021a61caa5c55...                  6   \n",
       "6   47580f85d0dd81692165c417576f6c4cf7b015ec21838c...                  7   \n",
       "7   30ba7af0e052a9f08830b2853c6777672a88fcb6ae3ec7...                  8   \n",
       "8   a2a423b48b790e6d9511ccd340618421bf574e7e866d55...                  9   \n",
       "9   e7adb4654819efd85be45646189342bc909210a39f4c95...                 10   \n",
       "10  78f47fc443a6a2604aae03e57a5aa1f49b76b2b51b7383...                 11   \n",
       "11  156d6e82c11ac3103488c6a3fc447ad0e7298b51658d9a...                 12   \n",
       "12  70b1bfed90e242fa52c3994e704045825393b7bd5c7efe...                 13   \n",
       "13  f17468e0cda42033623b8944e6f772ffcedf89d7043214...                 14   \n",
       "14  20f6d70a1845feeac80bcc3ada4c2808cc5a17989bb255...                 15   \n",
       "15  3516d752d4e2e9650243cd7c60ae6c1fe0d7dff3b7341b...                 16   \n",
       "16  6b0e4f7bbc8158a123cabf889a5eaa21984ff984653071...                 17   \n",
       "17  d4050c0d097eb36fe541c3bd14dc4b4025db8399231738...                 18   \n",
       "18  c6a8fc1e4491134e42c2c6ba630434d33b7d7414c5b9a3...                 19   \n",
       "19  24c77ef5b5fdff898d1b09fb813b3ea3613a0b98752de3...                 20   \n",
       "20  1cff7a8de1d7c42c38392088bc42a5577ee64bc1bdf217...                 21   \n",
       "21  67833d1f76c62bca92b938fb7d680b0ec2303719cf76b3...                 22   \n",
       "22  bd8f828f9196b7c7c8b5b1bbb130c31c419e9105ac99ba...                 23   \n",
       "23  9f3a4fddd9b13bb2af5edc2c844d6b38cbb038f9330661...                 24   \n",
       "24  7b58e4c9b1b81e0032bf09cf3305c73e39ba151af40602...                 25   \n",
       "25  253082c58a8e7cf7330cf38296e620108f02d6a05f6eb3...                 26   \n",
       "26  ad31c63c1ca6a1f679fbbfcad23e44035953fcb759fc47...                 27   \n",
       "27  7545e407be21a6c05c05035ab3d90113699580ec998fb1...                 28   \n",
       "28  3291681cefdb3df93a491259dd0c6ca86e3aed2809d8d9...                 29   \n",
       "\n",
       "                                                 text  n_tokens  \\\n",
       "0   ﻿The Project Gutenberg eBook of How to Invest ...      1200   \n",
       "1    way in which to dispose of it. It is obviousl...      1200   \n",
       "2   \\nmore than obedience to the old rule which fo...      1200   \n",
       "3    terms of a lease, by the railroads\\nwhich use...      1200   \n",
       "4   ust.\\n\\n\\n\\n\\nII\\n\\nRAILROAD MORTGAGE BONDS\\n\\...      1200   \n",
       "5   , to mortgage bonds upon the\\ngeneral mileage ...      1200   \n",
       "6   , it does not always follow that its operating...      1200   \n",
       "7    outstanding April 1st, 1908, at the market pr...      1200   \n",
       "8    to one of two standard forms: (1) The conditi...      1200   \n",
       "9   both. Two of these railroads offered to the ho...      1200   \n",
       "10  -1/4-per-cent loss in principal. If it were ne...      1200   \n",
       "11   such\\nincrease goes directly to the benefit o...      1200   \n",
       "12   in quoted values are\\nof no great importance ...      1200   \n",
       "13  , irrespective of the buildings and machinery ...      1200   \n",
       "14   grows, to\\naccumulate the additional amount n...      1200   \n",
       "15  vement in intrinsic value--are all inherent ch...      1200   \n",
       "16   to make a careful valuation of the other phys...      1200   \n",
       "17   be learned whether any real ground of content...      1200   \n",
       "18   years of depression. At the same time,\\nit sh...      1200   \n",
       "19   the tax lien is the foundation of the prime p...      1200   \n",
       "20   to the requirements of the business surplus? ...      1200   \n",
       "21  -managers attempted to give their stock an inv...      1200   \n",
       "22   swings of prices. If an investor acquires suc...      1200   \n",
       "23  produce general swings in prices which may be ...      1200   \n",
       "24   the excess finds its way into bank vaults\\nan...      1200   \n",
       "25   license, including paying royalties for use\\n...      1200   \n",
       "26   charges. If you are\\nredistributing or provid...      1200   \n",
       "27  , DISCLAIMER OF DAMAGES - Except for the “Righ...      1200   \n",
       "28   machine-readable form accessible by the wides...       467   \n",
       "\n",
       "                                         document_ids  \\\n",
       "0   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "1   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "2   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "3   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "4   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "5   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "6   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "7   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "8   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "9   [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "10  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "11  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "12  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "13  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "14  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "15  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "16  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "17  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "18  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "19  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "20  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "21  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "22  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "23  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "24  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "25  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "26  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "27  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "28  [76b874f8ddf312c5c5d34517a4587852e8ef79438600b...   \n",
       "\n",
       "                                           entity_ids  \\\n",
       "0   [bf9509fe-2d8b-429c-9a43-79c0d497b785, abcd315...   \n",
       "1                                                None   \n",
       "2                                                None   \n",
       "3                                                None   \n",
       "4                                                None   \n",
       "5                                                None   \n",
       "6                                                None   \n",
       "7                                                None   \n",
       "8                                                None   \n",
       "9                                                None   \n",
       "10                                               None   \n",
       "11                                               None   \n",
       "12                                               None   \n",
       "13                                               None   \n",
       "14                                               None   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                                               None   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21                                               None   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "26  [bf9509fe-2d8b-429c-9a43-79c0d497b785, e0cd2bc...   \n",
       "27  [bf9509fe-2d8b-429c-9a43-79c0d497b785, e0cd2bc...   \n",
       "28  [bf9509fe-2d8b-429c-9a43-79c0d497b785, 6d15953...   \n",
       "\n",
       "                                     relationship_ids covariate_ids  \n",
       "0   [4931c2ec-2b8c-4c38-97b2-d5ccd2647e6b, 33d0a3e...            []  \n",
       "1                                                None            []  \n",
       "2                                                None            []  \n",
       "3                                                None            []  \n",
       "4                                                None            []  \n",
       "5                                                None            []  \n",
       "6                                                None            []  \n",
       "7                                                None            []  \n",
       "8                                                None            []  \n",
       "9                                                None            []  \n",
       "10                                               None            []  \n",
       "11                                               None            []  \n",
       "12                                               None            []  \n",
       "13                                               None            []  \n",
       "14                                               None            []  \n",
       "15                                               None            []  \n",
       "16                                               None            []  \n",
       "17                                               None            []  \n",
       "18                                               None            []  \n",
       "19                                               None            []  \n",
       "20                                               None            []  \n",
       "21                                               None            []  \n",
       "22                                               None            []  \n",
       "23                                               None            []  \n",
       "24                                               None            []  \n",
       "25                                               None            []  \n",
       "26             [c936fe32-7873-40d7-b326-db8d8d66dc62]            []  \n",
       "27  [e0a4b47d-ab3c-421f-ad02-6e484c3c1b38, cb7497f...            []  \n",
       "28  [c936fe32-7873-40d7-b326-db8d8d66dc62, 30c0986...            []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_units_check.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j_env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
